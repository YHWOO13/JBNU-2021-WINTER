{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 3 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "f \n",
      " [[ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "Loss: 230\n",
      "{'W': array([[160],\n",
      "       [120],\n",
      "       [180]]), 'B': array([60])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.94]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 4 ~ 7 열\n",
      "N \n",
      " [[ 8.42]\n",
      " [ 9.26]\n",
      " [10.1 ]\n",
      " [10.94]]\n",
      "f \n",
      " [[ 9.36]\n",
      " [10.2 ]\n",
      " [11.04]\n",
      " [11.88]]\n",
      "Loss: 373.70559999999995\n",
      "{'W': array([[508.64],\n",
      "       [153.92],\n",
      "       [230.88]]), 'B': array([76.96])}\n",
      "W\n",
      "before\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "after\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94]]\n",
      "after\n",
      "[[0.86304]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 8 ~ 11 열\n",
      "N \n",
      " [[6.20176]]\n",
      "f \n",
      " [[7.0648]]\n",
      "Loss: 36.78179904\n",
      "{'W': array([[109.1664],\n",
      "       [ 24.2592],\n",
      "       [ 36.3888]]), 'B': array([12.1296])}\n",
      "W\n",
      "before\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "after\n",
      "[[0.2221936]\n",
      " [0.7018208]\n",
      " [0.5527312]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.86304]]\n",
      "after\n",
      "[[0.8509104]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 4\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 early stopping 추가한 Algorithm of mini-batch SGD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\n",
    "real data에서 모델의 성능이 낮아지는 경향을 막기 위해서 <br>\n",
    "\n",
    "1. 데이터를 training data, validation data, test data로 나눈다 <br>\n",
    "2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것 <br>\n",
    "3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다) <br>\n",
    "4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Data_set\n",
      "[[ -0.75453007  -0.62932287]\n",
      " [  7.42627036 -12.97321741]\n",
      " [  6.82548288 -10.96427286]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(N,size):\n",
    "    #size==특징의 개수\n",
    "    R = 10\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result=data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print('test_Data_set')\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.9958]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.9102]\n",
      " [ 9.9014]\n",
      " [10.8926]]\n",
      "f \n",
      " [[ 9.906 ]\n",
      " [10.8972]\n",
      " [11.8884]]\n",
      "Loss: 295.8286584\n",
      "{'W': array([[300.8808],\n",
      "       [118.7664],\n",
      "       [178.1496]]), 'B': array([59.3832])}\n",
      "W\n",
      "before\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "after\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9958]]\n",
      "after\n",
      "[[0.98986168]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[11.59598528]\n",
      " [12.5570972 ]\n",
      " [13.51820912]]\n",
      "f \n",
      " [[12.58584696]\n",
      " [13.54695888]\n",
      " [14.5080708 ]]\n",
      "Loss: 474.1260036547649\n",
      "{'W': array([[606.09847392],\n",
      "       [150.56350656],\n",
      "       [225.84525984]]), 'B': array([75.28175328])}\n",
      "W\n",
      "before\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "after\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.98986168]]\n",
      "after\n",
      "[[0.9823335]]\n",
      "\n",
      "\n",
      "******* 2 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.67083763]\n",
      " [6.57133971]\n",
      " [7.47184178]]\n",
      "f \n",
      " [[6.65317114]\n",
      " [7.55367321]\n",
      " [8.45417528]]\n",
      "Loss: 130.47370562049468\n",
      "{'W': array([[ 82.24608682],\n",
      "       [ 78.64407853],\n",
      "       [117.96611779]]), 'B': array([39.32203926])}\n",
      "W\n",
      "before\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "after\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9823335]]\n",
      "after\n",
      "[[0.9784013]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.28832677]\n",
      " [ 9.18060423]\n",
      " [10.07288169]]\n",
      "f \n",
      " [[ 9.26672807]\n",
      " [10.15900553]\n",
      " [11.05128299]]\n",
      "Loss: 253.25446504480863\n",
      "{'W': array([[278.33927576],\n",
      "       [109.90806636],\n",
      "       [164.86209954]]), 'B': array([54.95403318])}\n",
      "W\n",
      "before\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "after\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9784013]]\n",
      "after\n",
      "[[0.9729059]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[10.69888142]\n",
      " [11.56332496]\n",
      " [12.42776849]]\n",
      "f \n",
      " [[11.67178732]\n",
      " [12.53623085]\n",
      " [13.40067439]]\n",
      "Loss: 400.74839226007464\n",
      "{'W': array([[557.19685518],\n",
      "       [138.43477026],\n",
      "       [207.65215539]]), 'B': array([69.21738513])}\n",
      "W\n",
      "before\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "after\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9729059]]\n",
      "after\n",
      "[[0.96598416]]\n",
      "\n",
      "\n",
      "******* 3 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.36651792]\n",
      " [6.17524177]\n",
      " [6.98396562]]\n",
      "f \n",
      " [[6.33250208]\n",
      " [7.14122593]\n",
      " [7.94994978]]\n",
      "Loss: 114.45203617371786\n",
      "{'W': array([[ 76.92960652],\n",
      "       [ 73.69471112],\n",
      "       [110.54206668]]), 'B': array([36.84735556])}\n",
      "W\n",
      "before\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "after\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96598416]]\n",
      "after\n",
      "[[0.96229942]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.71401606]\n",
      " [8.51504695]\n",
      " [9.31607784]]\n",
      "f \n",
      " [[ 8.67631549]\n",
      " [ 9.47734638]\n",
      " [10.27837727]]\n",
      "Loss: 216.87950580813276\n",
      "{'W': array([[257.5245149 ],\n",
      "       [101.72815654],\n",
      "       [152.5922348 ]]), 'B': array([50.86407827])}\n",
      "W\n",
      "before\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "after\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96229942]]\n",
      "after\n",
      "[[0.95721302]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.87071827]\n",
      " [10.64599671]\n",
      " [11.42127515]]\n",
      "f \n",
      " [[10.82793129]\n",
      " [11.60320973]\n",
      " [12.37848817]]\n",
      "Loss: 338.4862828577476\n",
      "{'W': array([[512.05518065],\n",
      "       [127.23851672],\n",
      "       [190.85777509]]), 'B': array([63.61925836])}\n",
      "W\n",
      "before\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "after\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95721302]]\n",
      "after\n",
      "[[0.95085109]]\n",
      "\n",
      "\n",
      "******* 4 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.08513709]\n",
      " [5.80921001]\n",
      " [6.53328293]]\n",
      "f \n",
      " [[6.03598818]\n",
      " [6.7600611 ]\n",
      " [7.48413402]]\n",
      "Loss: 100.58347471209066\n",
      "{'W': array([[ 72.01702485],\n",
      "       [ 69.12073316],\n",
      "       [103.68109975]]), 'B': array([34.56036658])}\n",
      "W\n",
      "before\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "after\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95085109]]\n",
      "after\n",
      "[[0.94739505]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.18362056]\n",
      " [7.90049178]\n",
      " [8.617363  ]]\n",
      "f \n",
      " [[8.13101562]\n",
      " [8.84788683]\n",
      " [9.56475805]]\n",
      "Loss: 185.7957919285563\n",
      "{'W': array([[238.30408987],\n",
      "       [ 94.174642  ],\n",
      "       [141.261963  ]]), 'B': array([47.087321])}\n",
      "W\n",
      "before\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "after\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94739505]]\n",
      "after\n",
      "[[0.94268632]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.10620784]\n",
      " [ 9.79924865]\n",
      " [10.49228945]]\n",
      "f \n",
      " [[10.04889416]\n",
      " [10.74193497]\n",
      " [11.43497578]]\n",
      "Loss: 285.6765017928727\n",
      "{'W': array([[470.38504162],\n",
      "       [116.9032196 ],\n",
      "       [175.35482939]]), 'B': array([58.4516098])}\n",
      "W\n",
      "before\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "after\n",
      "[[0.6460023 ]\n",
      " [0.87368232]\n",
      " [0.81052348]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94268632]]\n",
      "after\n",
      "[[0.93684116]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.4.3 초기버전\n",
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "epoch_size=4\n",
    "\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        loss_gradient(matrix1,y1,weights)\n",
    "\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.9689401938930493\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.9685654731621512\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.9682920487893232\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.9681222431793726\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.9680685936744505\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.9679714942295727\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.9678958249531701\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.9678384748345579\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.9677654888726896\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.9676859024417606\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.9676210750366333\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.9675603487064173\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.9675091799574722\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.9674612349463612\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.9674016822582777\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.9673491893169696\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.9672842793556077\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.9672294521133468\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.9671920656044327\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.9671434335381993\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.967090997289546\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.9670408725115383\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.967010149131426\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.9669715744367768\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.9669170061837635\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.9668776534959732\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.9668461854501592\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.9668211485165201\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.9667842538106083\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.9667498675140865\n",
      "\n",
      "Test_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.8460437772521384\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.8458543671115325\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.8456912271254238\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.8455398469656317\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.8454043653043112\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.8452769233868945\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.8451841598138468\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.8450932142291202\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.8450024778234119\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.8449280586853085\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.844865691490416\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.8448074624156071\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.8447666573253051\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.8447230225418021\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.8446755581765079\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.8446389745376288\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.8446080089355573\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.8445808389582693\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.8445532517743298\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.844530515675891\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.8445074861359048\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.844486527000699\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.844472541910597\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.8444563334778973\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.8444411450994238\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.8444287311678326\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.8444184255300982\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.8444101616351393\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.8444018849391614\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.8443927409833407\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "xlis = []\n",
    "ylis = []\n",
    "\n",
    "R=10\n",
    "size=1\n",
    "weights={}\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "batch={}\n",
    "\n",
    "W= np.random.uniform(-R,R,size=size)\n",
    "b= np.random.uniform(-R,R,size=size)\n",
    "b= random.choice(b)\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.random.uniform(-R,R,size=size)\n",
    "    y = np.random.normal(W*x+b,1,size=size)\n",
    "    xlis.append(x)\n",
    "    ylis.append(y)\n",
    "    flis.append(W*x+b)\n",
    "\n",
    "x=np.array(xlis)\n",
    "y=np.array(ylis)\n",
    "\n",
    "weights['W']=W\n",
    "weights['B']=b\n",
    "\n",
    "result=np.concatenate((x,y),axis=1)\n",
    "\n",
    "train_idx=int(result.shape[0]*0.85)\n",
    "dev_idx=int(result.shape[0]*0.05)\n",
    "test_idx=int(result.shape[0]*0.1)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "def linear_regression(data, idx, minibatch_size, epoch_size):\n",
    "    data_list=[]\n",
    "    X_batch= data[:,0]\n",
    "    y_batch= data[:,1]\n",
    "    X_batch=np.reshape(X_batch,(idx,size))\n",
    "    y_batch=np.reshape(y_batch,(idx,size))\n",
    "\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*************',j,'번차 epoch *************')\n",
    "        data=np.random.permutation(data)\n",
    "        X_batch= data[:,0]\n",
    "        y_batch= data[:,1]\n",
    "        X_batch=np.reshape(X_batch,(idx,size))\n",
    "        y_batch=np.reshape(y_batch,(idx,size))\n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            X_batch_temp=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y_temp=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            N=weights['W']*X_batch_temp\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y_temp-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch_temp\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y_temp # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.00001 * loss_grad[key]\n",
    "        \n",
    "        N=weights['W']*X_batch\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y_batch-f,2))\n",
    "        print('Loss',loss)\n",
    "      \n",
    "        #print('=================================')\n",
    "        \n",
    "        data_list.append(loss)\n",
    "        \n",
    "    X_batch = list(X_batch)\n",
    "    \n",
    "    #epoch에 따른 loss 출력도 가능\n",
    "    return plt.scatter(X_batch_temp, y_temp, label = 'name')\n",
    "\n",
    "print('Train_data')\n",
    "train_ = linear_regression(train_data_set,train_idx,50,30)\n",
    "\n",
    "#print('\\ndev_data')\n",
    "#dev_ = linear_regression(dev_data_set,dev_idx,20,30)\n",
    "\n",
    "print('\\nTest_data')\n",
    "Test_ = linear_regression(test_data_set,test_idx,50,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
