{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 -2 -7]\n",
      " [-4  4 -4]]\n",
      "\n",
      " [[ 5 -4]\n",
      " [-2  4]\n",
      " [-7 -4]]\n",
      "\n",
      " [[ 0  2]\n",
      " [-4 -2]]\n",
      "\n",
      " [[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Linear regression을 위한 식 유도 (행렬에 대한 미분공식 이용)\n",
    "np.random.seed(0)\n",
    "w_row=random.randrange(2,4)\n",
    "w_col=random.randrange(2,4)\n",
    "x_col=random.randrange(2,4)\n",
    "\n",
    "w=np.random.randint(-7,7,(w_row,w_col))\n",
    "w_T=np.transpose(w)\n",
    "x=np.random.randint(-7,7,(w_row,x_col))\n",
    "bias=np.random.randint(0,1,(x_col,1))\n",
    "bias=np.ones_like(bias)\n",
    "\n",
    "print(w)\n",
    "print('\\n',w_T)\n",
    "print('\\n',x)\n",
    "print('\\n',bias)\n",
    "\n",
    "# 2.1-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.958]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[8.102]\n",
      " [9.014]\n",
      " [9.926]]\n",
      "f \n",
      " [[ 9.06 ]\n",
      " [ 9.972]\n",
      " [10.884]]\n",
      "Loss: 243.15384\n",
      "{'W': array([[272.808],\n",
      "       [107.664],\n",
      "       [161.496]]), 'B': array([53.832])}\n",
      "W\n",
      "before\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "after\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.958]]\n",
      "after\n",
      "[[0.904168]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[8.228528]\n",
      " [8.86772 ]\n",
      " [9.506912]]\n",
      "f \n",
      " [[ 9.132696]\n",
      " [ 9.771888]\n",
      " [10.41108 ]]\n",
      "Loss: 231.65519007936\n",
      "{'W': array([[423.607392],\n",
      "       [105.262656],\n",
      "       [157.893984]]), 'B': array([52.631328])}\n",
      "W\n",
      "before\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "after\n",
      "[[0.21558461]\n",
      " [0.70307334]\n",
      " [0.55461002]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.904168]]\n",
      "after\n",
      "[[0.85153667]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17fc62dc1269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mdev_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtest_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(R,size):\n",
    "    #size==특징의 개수\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(30):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "\n",
    "        \n",
    "data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)\n",
    "print(test_data_set)\n",
    "\n",
    "#print(result[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "미니배치 사이즈를 입력(10~100): 30\n",
      "epoch size(최대 100): 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bc7b0d616f49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch size(최대 100):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mminibatch_size\u001b[0m \u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mloss_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_x' is not defined"
     ]
    }
   ],
   "source": [
    "''' w,b를 찾아 가는 것. x와 y는 한세트라고 보면 된다\n",
    "    따라서, 셔플을 할 때 x와y로 이루어진 하나의 셋에 셔플을 주는 것이다.'''\n",
    "def data_set(R,size):\n",
    "  #  R=int(input('range: '))\n",
    " #   dimension_size=int(input('size: '))\n",
    "#    sample=int(input('sample: '))\n",
    "\n",
    "    xlis = []\n",
    "    ylis = []\n",
    "    wlis = []\n",
    "    \n",
    "    W= np.random.uniform(-R,R,size=size) #size==특징\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "    \n",
    "    mu=W*x+b\n",
    "    sigma=0.1*R\n",
    "\n",
    "    for i in range(10):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(mu,sigma,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "    \n",
    "x1=np.array(xlis)\n",
    "y1=np.array(ylis)\n",
    "\n",
    "'''\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')'''\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    \n",
    "    loss_grad={}    # dJdW, dJdB 저장공간\n",
    "    forward_info={} # 순방향 저장공간\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    loss(X_batch, y_batch, W, b)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    print(number_minibatch)\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "                \n",
    "                \n",
    "            print(loss)\n",
    "            print('=================================')\n",
    "             #print(loss_grad)\n",
    "\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        loss_gradient(train_data_x,train_data_y,train_data_W,train_data_b)\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3 Linear regression 학습기 Practice\n",
    "\n",
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "b=np.reshape(b,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:860,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:860,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:860,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+train_data_b\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+dev_data_b\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+test_data_b\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=10)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(10,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b, minibatch_size, epoch_size):    \n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "\n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dJdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dJdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "\n",
    "    print(weights['W'])\n",
    "        #print(loss_grad)\n",
    "'''\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        \n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue\n",
    "'''\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    W=np.random.permutation(W)\n",
    "    b=np.random.permutation(b)\n",
    "    x=np.random.permutation(x)\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b,minibatch_size, epoch_size)\n",
    "        for key in weights.keys():\n",
    "\n",
    "            weights[key]=weights[key]- 0.01 * loss_grad[key]\n",
    "    print(weights['W'])\n",
    "    print('LOSS:',loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "b1=random.choice(b)\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:900,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:900,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "'''\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:900,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "'''\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+b1\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+b1\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+b1\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=50)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(50,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "    \n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "    return loss_grad\n",
    "\n",
    "#loss(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "\n",
    "def epoch(x,y,W,b,minibatch_size,epoch_size):\n",
    "    number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "    weights = {'W':W,'B':b} \n",
    "    data_list=[]\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        W=np.random.permutation(W)\n",
    "        x=np.random.permutation(x)\n",
    "        b1=random.choice(b)\n",
    "\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            #print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            '''\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            '''\n",
    "            x1=x[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "           \n",
    "            loss_gradient(x1,y1,W1,b1)\n",
    "            \n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "        print('Loss:',loss(x,y,W,b)*0.00001)\n",
    "        data_list.append(loss(x,y,W,b)*0.00001)\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "print('Dev')\n",
    "dev_data=epoch(dev_data_x,dev_data_y,dev_data_W,b,minibatch_size,epoch_size)\n",
    "print('\\nTest')\n",
    "Test_data=epoch(test_data_x,test_data_y,test_data_W,test_data_b,minibatch_size,epoch_size)        \n",
    "print('\\nTrain')\n",
    "Train_data=epoch(train_data_x,train_data_y,train_data_W,train_data_b,minibatch_size,epoch_size)\n",
    "\n",
    "plt.plot(dev_data, label='dev_loss')\n",
    "plt.plot(Test_data, label='Test_loss')\n",
    "plt.plot(Train_data, label='Train_loss')\n",
    "plt.xlabel('Max_iter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
