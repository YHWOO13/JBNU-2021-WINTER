{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 3 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "f \n",
      " [[ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "Loss: 230\n",
      "{'W': array([[160],\n",
      "       [120],\n",
      "       [180]]), 'B': array([60])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.94]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 4 ~ 7 열\n",
      "N \n",
      " [[ 8.42]\n",
      " [ 9.26]\n",
      " [10.1 ]\n",
      " [10.94]]\n",
      "f \n",
      " [[ 9.36]\n",
      " [10.2 ]\n",
      " [11.04]\n",
      " [11.88]]\n",
      "Loss: 373.70559999999995\n",
      "{'W': array([[508.64],\n",
      "       [153.92],\n",
      "       [230.88]]), 'B': array([76.96])}\n",
      "W\n",
      "before\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "after\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94]]\n",
      "after\n",
      "[[0.86304]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 8 ~ 11 열\n",
      "N \n",
      " [[6.20176]]\n",
      "f \n",
      " [[7.0648]]\n",
      "Loss: 36.78179904\n",
      "{'W': array([[109.1664],\n",
      "       [ 24.2592],\n",
      "       [ 36.3888]]), 'B': array([12.1296])}\n",
      "W\n",
      "before\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "after\n",
      "[[0.2221936]\n",
      " [0.7018208]\n",
      " [0.5527312]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.86304]]\n",
      "after\n",
      "[[0.8509104]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 4\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 early stopping 추가한 Algorithm of mini-batch SGD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\n",
    "real data에서 모델의 성능이 낮아지는 경향을 막기 위해서 <br>\n",
    "\n",
    "1. 데이터를 training data, validation data, test data로 나눈다 <br>\n",
    "2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것 <br>\n",
    "3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다) <br>\n",
    "4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Data_set\n",
      "[[-8.89447941  6.65732007]\n",
      " [-9.58643423  5.66127989]\n",
      " [-2.44091189  4.36799296]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(N,size):\n",
    "    #size==특징의 개수\n",
    "    R = 10\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result=data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print('test_Data_set')\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.9958]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.9102]\n",
      " [ 9.9014]\n",
      " [10.8926]]\n",
      "f \n",
      " [[ 9.906 ]\n",
      " [10.8972]\n",
      " [11.8884]]\n",
      "Loss: 295.8286584\n",
      "{'W': array([[300.8808],\n",
      "       [118.7664],\n",
      "       [178.1496]]), 'B': array([59.3832])}\n",
      "W\n",
      "before\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "after\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9958]]\n",
      "after\n",
      "[[0.98986168]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[11.59598528]\n",
      " [12.5570972 ]\n",
      " [13.51820912]]\n",
      "f \n",
      " [[12.58584696]\n",
      " [13.54695888]\n",
      " [14.5080708 ]]\n",
      "Loss: 474.1260036547649\n",
      "{'W': array([[606.09847392],\n",
      "       [150.56350656],\n",
      "       [225.84525984]]), 'B': array([75.28175328])}\n",
      "W\n",
      "before\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "after\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.98986168]]\n",
      "after\n",
      "[[0.9823335]]\n",
      "\n",
      "\n",
      "******* 2 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.67083763]\n",
      " [6.57133971]\n",
      " [7.47184178]]\n",
      "f \n",
      " [[6.65317114]\n",
      " [7.55367321]\n",
      " [8.45417528]]\n",
      "Loss: 130.47370562049468\n",
      "{'W': array([[ 82.24608682],\n",
      "       [ 78.64407853],\n",
      "       [117.96611779]]), 'B': array([39.32203926])}\n",
      "W\n",
      "before\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "after\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9823335]]\n",
      "after\n",
      "[[0.9784013]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.28832677]\n",
      " [ 9.18060423]\n",
      " [10.07288169]]\n",
      "f \n",
      " [[ 9.26672807]\n",
      " [10.15900553]\n",
      " [11.05128299]]\n",
      "Loss: 253.25446504480863\n",
      "{'W': array([[278.33927576],\n",
      "       [109.90806636],\n",
      "       [164.86209954]]), 'B': array([54.95403318])}\n",
      "W\n",
      "before\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "after\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9784013]]\n",
      "after\n",
      "[[0.9729059]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[10.69888142]\n",
      " [11.56332496]\n",
      " [12.42776849]]\n",
      "f \n",
      " [[11.67178732]\n",
      " [12.53623085]\n",
      " [13.40067439]]\n",
      "Loss: 400.74839226007464\n",
      "{'W': array([[557.19685518],\n",
      "       [138.43477026],\n",
      "       [207.65215539]]), 'B': array([69.21738513])}\n",
      "W\n",
      "before\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "after\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9729059]]\n",
      "after\n",
      "[[0.96598416]]\n",
      "\n",
      "\n",
      "******* 3 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.36651792]\n",
      " [6.17524177]\n",
      " [6.98396562]]\n",
      "f \n",
      " [[6.33250208]\n",
      " [7.14122593]\n",
      " [7.94994978]]\n",
      "Loss: 114.45203617371786\n",
      "{'W': array([[ 76.92960652],\n",
      "       [ 73.69471112],\n",
      "       [110.54206668]]), 'B': array([36.84735556])}\n",
      "W\n",
      "before\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "after\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96598416]]\n",
      "after\n",
      "[[0.96229942]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.71401606]\n",
      " [8.51504695]\n",
      " [9.31607784]]\n",
      "f \n",
      " [[ 8.67631549]\n",
      " [ 9.47734638]\n",
      " [10.27837727]]\n",
      "Loss: 216.87950580813276\n",
      "{'W': array([[257.5245149 ],\n",
      "       [101.72815654],\n",
      "       [152.5922348 ]]), 'B': array([50.86407827])}\n",
      "W\n",
      "before\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "after\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96229942]]\n",
      "after\n",
      "[[0.95721302]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.87071827]\n",
      " [10.64599671]\n",
      " [11.42127515]]\n",
      "f \n",
      " [[10.82793129]\n",
      " [11.60320973]\n",
      " [12.37848817]]\n",
      "Loss: 338.4862828577476\n",
      "{'W': array([[512.05518065],\n",
      "       [127.23851672],\n",
      "       [190.85777509]]), 'B': array([63.61925836])}\n",
      "W\n",
      "before\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "after\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95721302]]\n",
      "after\n",
      "[[0.95085109]]\n",
      "\n",
      "\n",
      "******* 4 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.08513709]\n",
      " [5.80921001]\n",
      " [6.53328293]]\n",
      "f \n",
      " [[6.03598818]\n",
      " [6.7600611 ]\n",
      " [7.48413402]]\n",
      "Loss: 100.58347471209066\n",
      "{'W': array([[ 72.01702485],\n",
      "       [ 69.12073316],\n",
      "       [103.68109975]]), 'B': array([34.56036658])}\n",
      "W\n",
      "before\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "after\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95085109]]\n",
      "after\n",
      "[[0.94739505]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.18362056]\n",
      " [7.90049178]\n",
      " [8.617363  ]]\n",
      "f \n",
      " [[8.13101562]\n",
      " [8.84788683]\n",
      " [9.56475805]]\n",
      "Loss: 185.7957919285563\n",
      "{'W': array([[238.30408987],\n",
      "       [ 94.174642  ],\n",
      "       [141.261963  ]]), 'B': array([47.087321])}\n",
      "W\n",
      "before\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "after\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94739505]]\n",
      "after\n",
      "[[0.94268632]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.10620784]\n",
      " [ 9.79924865]\n",
      " [10.49228945]]\n",
      "f \n",
      " [[10.04889416]\n",
      " [10.74193497]\n",
      " [11.43497578]]\n",
      "Loss: 285.6765017928727\n",
      "{'W': array([[470.38504162],\n",
      "       [116.9032196 ],\n",
      "       [175.35482939]]), 'B': array([58.4516098])}\n",
      "W\n",
      "before\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "after\n",
      "[[0.6460023 ]\n",
      " [0.87368232]\n",
      " [0.81052348]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94268632]]\n",
      "after\n",
      "[[0.93684116]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.4.3 초기버전\n",
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "epoch_size=4\n",
    "\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        loss_gradient(matrix1,y1,weights)\n",
    "\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.9235630168555412\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.9233670725735574\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.9232985224576715\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.9232787741671946\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.9233096448939798\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.9233112186360235\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.9232837266378914\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.9232935103450938\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.9232750327200683\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.9232753880339735\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.9232736000756818\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.9232741364205345\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.9233262661043549\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.9232753466507722\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.9232717948551896\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.9232751414208756\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.9232792388434902\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.9232767594994784\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.9233268159661001\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.923322033340148\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.9232756452991043\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.9232728570458337\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.923286166970902\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.9232718614031552\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.9232717937479229\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.9232935824494943\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.9232764299746227\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.9232726773107653\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.9232743936983488\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.923272813062534\n",
      "\n",
      "Test_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 1.0118260997356914\n",
      "************* 2 번차 epoch *************\n",
      "Loss 1.0113893205919973\n",
      "************* 3 번차 epoch *************\n",
      "Loss 1.0109948590191011\n",
      "************* 4 번차 epoch *************\n",
      "Loss 1.0106710713248062\n",
      "************* 5 번차 epoch *************\n",
      "Loss 1.010387503638499\n",
      "************* 6 번차 epoch *************\n",
      "Loss 1.0101517522864676\n",
      "************* 7 번차 epoch *************\n",
      "Loss 1.0099273332282155\n",
      "************* 8 번차 epoch *************\n",
      "Loss 1.0097295128622203\n",
      "************* 9 번차 epoch *************\n",
      "Loss 1.0095484612068137\n",
      "************* 10 번차 epoch *************\n",
      "Loss 1.0093865670439965\n",
      "************* 11 번차 epoch *************\n",
      "Loss 1.0092682744163826\n",
      "************* 12 번차 epoch *************\n",
      "Loss 1.0091590978320364\n",
      "************* 13 번차 epoch *************\n",
      "Loss 1.0090620417445182\n",
      "************* 14 번차 epoch *************\n",
      "Loss 1.0089760468146314\n",
      "************* 15 번차 epoch *************\n",
      "Loss 1.0089017114329946\n",
      "************* 16 번차 epoch *************\n",
      "Loss 1.0088439278400403\n",
      "************* 17 번차 epoch *************\n",
      "Loss 1.008784411965679\n",
      "************* 18 번차 epoch *************\n",
      "Loss 1.0087387578138312\n",
      "************* 19 번차 epoch *************\n",
      "Loss 1.0087016631408314\n",
      "************* 20 번차 epoch *************\n",
      "Loss 1.0086613342927127\n",
      "************* 21 번차 epoch *************\n",
      "Loss 1.0086327003731692\n",
      "************* 22 번차 epoch *************\n",
      "Loss 1.008604341108773\n",
      "************* 23 번차 epoch *************\n",
      "Loss 1.0085809144409625\n",
      "************* 24 번차 epoch *************\n",
      "Loss 1.0085562107762756\n",
      "************* 25 번차 epoch *************\n",
      "Loss 1.0085413923837583\n",
      "************* 26 번차 epoch *************\n",
      "Loss 1.008520390524001\n",
      "************* 27 번차 epoch *************\n",
      "Loss 1.0085110779052353\n",
      "************* 28 번차 epoch *************\n",
      "Loss 1.0084966937823077\n",
      "************* 29 번차 epoch *************\n",
      "Loss 1.0084832681826643\n",
      "************* 30 번차 epoch *************\n",
      "Loss 1.008470336254735\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "xlis = []\n",
    "ylis = []\n",
    "\n",
    "R=10\n",
    "size=1\n",
    "weights={}\n",
    "#data_set={}    \n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "batch={}\n",
    "\n",
    "W= np.random.uniform(-R,R,size=size)\n",
    "b= np.random.uniform(-R,R,size=size)\n",
    "b= random.choice(b)\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.random.uniform(-R,R,size=size)\n",
    "    y = np.random.normal(W*x+b,1,size=size)\n",
    "    xlis.append(x)\n",
    "    ylis.append(y)\n",
    "    flis.append(W*x+b)\n",
    "\n",
    "x=np.array(xlis)\n",
    "y=np.array(ylis)\n",
    "\n",
    "weights['W']=W\n",
    "weights['B']=b\n",
    "\n",
    "result=np.concatenate((x,y),axis=1)\n",
    "\n",
    "train_idx=int(result.shape[0]*0.85)\n",
    "dev_idx=int(result.shape[0]*0.05)\n",
    "test_idx=int(result.shape[0]*0.1)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "def linear_regression(data, idx, minibatch_size, epoch_size):\n",
    "    data_list=[]\n",
    "    X_batch= data[:,0]\n",
    "    y_batch= data[:,1]\n",
    "    X_batch=np.reshape(X_batch,(idx,size))\n",
    "    y_batch=np.reshape(y_batch,(idx,size))\n",
    "\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*************',j,'번차 epoch *************')\n",
    "        data=np.random.permutation(data)\n",
    "        X_batch= data[:,0]\n",
    "        y_batch= data[:,1]\n",
    "        X_batch=np.reshape(X_batch,(idx,size))\n",
    "        y_batch=np.reshape(y_batch,(idx,size))\n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            X_batch_temp=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y_temp=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            N=weights['W']*X_batch_temp\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y_temp-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch_temp\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y_temp # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.00001 * loss_grad[key]\n",
    "        \n",
    "        N=weights['W']*X_batch\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y_batch-f,2))\n",
    "        print('Loss',loss)\n",
    "      \n",
    "        #print('=================================')\n",
    "        \n",
    "        data_list.append(loss)\n",
    "        \n",
    "    X_batch = list(X_batch)\n",
    "    \n",
    "    #epoch에 따른 loss 출력도 가능\n",
    "    return plt.scatter(X_batch_temp, y_temp, label = 'name')\n",
    "\n",
    "print('Train_data')\n",
    "train_ = linear_regression(train_data_set,train_idx,50,30)\n",
    "\n",
    "#print('\\ndev_data')\n",
    "#dev_ = linear_regression(dev_data_set,dev_idx,20,30)\n",
    "\n",
    "print('\\nTest_data')\n",
    "Test_ = linear_regression(test_data_set,test_idx,50,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
