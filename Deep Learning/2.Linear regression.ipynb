{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 3 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "f \n",
      " [[ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "Loss: 230\n",
      "{'W': array([[160],\n",
      "       [120],\n",
      "       [180]]), 'B': array([60])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.94]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 4 ~ 7 열\n",
      "N \n",
      " [[ 8.42]\n",
      " [ 9.26]\n",
      " [10.1 ]\n",
      " [10.94]]\n",
      "f \n",
      " [[ 9.36]\n",
      " [10.2 ]\n",
      " [11.04]\n",
      " [11.88]]\n",
      "Loss: 373.70559999999995\n",
      "{'W': array([[508.64],\n",
      "       [153.92],\n",
      "       [230.88]]), 'B': array([76.96])}\n",
      "W\n",
      "before\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "after\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94]]\n",
      "after\n",
      "[[0.86304]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 8 ~ 11 열\n",
      "N \n",
      " [[6.20176]]\n",
      "f \n",
      " [[7.0648]]\n",
      "Loss: 36.78179904\n",
      "{'W': array([[109.1664],\n",
      "       [ 24.2592],\n",
      "       [ 36.3888]]), 'B': array([12.1296])}\n",
      "W\n",
      "before\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "after\n",
      "[[0.2221936]\n",
      " [0.7018208]\n",
      " [0.5527312]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.86304]]\n",
      "after\n",
      "[[0.8509104]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 4\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 early stopping 추가한 Algorithm of mini-batch SGD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\n",
    "real data에서 모델의 성능이 낮아지는 경향을 막기 위해서 <br>\n",
    "\n",
    "1. 데이터를 training data, validation data, test data로 나눈다 <br>\n",
    "2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것 <br>\n",
    "3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다) <br>\n",
    "4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Data_set\n",
      "[[-8.89447941  6.65732007]\n",
      " [-9.58643423  5.66127989]\n",
      " [-2.44091189  4.36799296]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(N,size):\n",
    "    #size==특징의 개수\n",
    "    R = 10\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result=data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print('test_Data_set')\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.9958]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.9102]\n",
      " [ 9.9014]\n",
      " [10.8926]]\n",
      "f \n",
      " [[ 9.906 ]\n",
      " [10.8972]\n",
      " [11.8884]]\n",
      "Loss: 295.8286584\n",
      "{'W': array([[300.8808],\n",
      "       [118.7664],\n",
      "       [178.1496]]), 'B': array([59.3832])}\n",
      "W\n",
      "before\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "after\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9958]]\n",
      "after\n",
      "[[0.98986168]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[11.59598528]\n",
      " [12.5570972 ]\n",
      " [13.51820912]]\n",
      "f \n",
      " [[12.58584696]\n",
      " [13.54695888]\n",
      " [14.5080708 ]]\n",
      "Loss: 474.1260036547649\n",
      "{'W': array([[606.09847392],\n",
      "       [150.56350656],\n",
      "       [225.84525984]]), 'B': array([75.28175328])}\n",
      "W\n",
      "before\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "after\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.98986168]]\n",
      "after\n",
      "[[0.9823335]]\n",
      "\n",
      "\n",
      "******* 2 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.67083763]\n",
      " [6.57133971]\n",
      " [7.47184178]]\n",
      "f \n",
      " [[6.65317114]\n",
      " [7.55367321]\n",
      " [8.45417528]]\n",
      "Loss: 130.47370562049468\n",
      "{'W': array([[ 82.24608682],\n",
      "       [ 78.64407853],\n",
      "       [117.96611779]]), 'B': array([39.32203926])}\n",
      "W\n",
      "before\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "after\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9823335]]\n",
      "after\n",
      "[[0.9784013]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.28832677]\n",
      " [ 9.18060423]\n",
      " [10.07288169]]\n",
      "f \n",
      " [[ 9.26672807]\n",
      " [10.15900553]\n",
      " [11.05128299]]\n",
      "Loss: 253.25446504480863\n",
      "{'W': array([[278.33927576],\n",
      "       [109.90806636],\n",
      "       [164.86209954]]), 'B': array([54.95403318])}\n",
      "W\n",
      "before\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "after\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9784013]]\n",
      "after\n",
      "[[0.9729059]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[10.69888142]\n",
      " [11.56332496]\n",
      " [12.42776849]]\n",
      "f \n",
      " [[11.67178732]\n",
      " [12.53623085]\n",
      " [13.40067439]]\n",
      "Loss: 400.74839226007464\n",
      "{'W': array([[557.19685518],\n",
      "       [138.43477026],\n",
      "       [207.65215539]]), 'B': array([69.21738513])}\n",
      "W\n",
      "before\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "after\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9729059]]\n",
      "after\n",
      "[[0.96598416]]\n",
      "\n",
      "\n",
      "******* 3 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.36651792]\n",
      " [6.17524177]\n",
      " [6.98396562]]\n",
      "f \n",
      " [[6.33250208]\n",
      " [7.14122593]\n",
      " [7.94994978]]\n",
      "Loss: 114.45203617371786\n",
      "{'W': array([[ 76.92960652],\n",
      "       [ 73.69471112],\n",
      "       [110.54206668]]), 'B': array([36.84735556])}\n",
      "W\n",
      "before\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "after\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96598416]]\n",
      "after\n",
      "[[0.96229942]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.71401606]\n",
      " [8.51504695]\n",
      " [9.31607784]]\n",
      "f \n",
      " [[ 8.67631549]\n",
      " [ 9.47734638]\n",
      " [10.27837727]]\n",
      "Loss: 216.87950580813276\n",
      "{'W': array([[257.5245149 ],\n",
      "       [101.72815654],\n",
      "       [152.5922348 ]]), 'B': array([50.86407827])}\n",
      "W\n",
      "before\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "after\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96229942]]\n",
      "after\n",
      "[[0.95721302]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.87071827]\n",
      " [10.64599671]\n",
      " [11.42127515]]\n",
      "f \n",
      " [[10.82793129]\n",
      " [11.60320973]\n",
      " [12.37848817]]\n",
      "Loss: 338.4862828577476\n",
      "{'W': array([[512.05518065],\n",
      "       [127.23851672],\n",
      "       [190.85777509]]), 'B': array([63.61925836])}\n",
      "W\n",
      "before\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "after\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95721302]]\n",
      "after\n",
      "[[0.95085109]]\n",
      "\n",
      "\n",
      "******* 4 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.08513709]\n",
      " [5.80921001]\n",
      " [6.53328293]]\n",
      "f \n",
      " [[6.03598818]\n",
      " [6.7600611 ]\n",
      " [7.48413402]]\n",
      "Loss: 100.58347471209066\n",
      "{'W': array([[ 72.01702485],\n",
      "       [ 69.12073316],\n",
      "       [103.68109975]]), 'B': array([34.56036658])}\n",
      "W\n",
      "before\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "after\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95085109]]\n",
      "after\n",
      "[[0.94739505]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.18362056]\n",
      " [7.90049178]\n",
      " [8.617363  ]]\n",
      "f \n",
      " [[8.13101562]\n",
      " [8.84788683]\n",
      " [9.56475805]]\n",
      "Loss: 185.7957919285563\n",
      "{'W': array([[238.30408987],\n",
      "       [ 94.174642  ],\n",
      "       [141.261963  ]]), 'B': array([47.087321])}\n",
      "W\n",
      "before\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "after\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94739505]]\n",
      "after\n",
      "[[0.94268632]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.10620784]\n",
      " [ 9.79924865]\n",
      " [10.49228945]]\n",
      "f \n",
      " [[10.04889416]\n",
      " [10.74193497]\n",
      " [11.43497578]]\n",
      "Loss: 285.6765017928727\n",
      "{'W': array([[470.38504162],\n",
      "       [116.9032196 ],\n",
      "       [175.35482939]]), 'B': array([58.4516098])}\n",
      "W\n",
      "before\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "after\n",
      "[[0.6460023 ]\n",
      " [0.87368232]\n",
      " [0.81052348]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94268632]]\n",
      "after\n",
      "[[0.93684116]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.4.3 초기버전\n",
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "epoch_size=4\n",
    "\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        loss_gradient(matrix1,y1,weights)\n",
    "\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.9269747649619349\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.9255529899045934\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.9252094745569874\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.9249329987262979\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.9248637872766967\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.9247551595762799\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.9246854296653801\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.9246194466505055\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.9245504649293509\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.9244870373990824\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.924431993940421\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.9243667227222093\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.9243143856002017\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.9242567008532245\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.9242125203659395\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.9241498139150682\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.9240983077102272\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.9240507699619884\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.9240076349992076\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.9239605428830949\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.9239143656630353\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.9238820429108706\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.923833710751722\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.9237927945951119\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.9237588578559673\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.9237297413727306\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.9236827568936843\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.9236521221794409\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.9236142772887618\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.9235852327236428\n",
      "\n",
      "Test_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.8428350450215071\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.8428160990853119\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.8427989588839143\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.8427879584560319\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.84277318169036\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.8427632650490725\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.8427526444081549\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.8427435453760553\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.842734051970024\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.8427244475386524\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.8427179395214659\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.8427134012962457\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.8427085842831546\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.8427029003176836\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.8426986600796017\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.8426951675488141\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.8426919276223528\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.8426888969736152\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.8426865142431574\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.8426841755883416\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.8426827723102714\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.8426819591372602\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.842680566275305\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.8426792777771059\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.8426770369908944\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.842675798444197\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.8426749603083578\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.8426737681216525\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.8426725436419003\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.8426714940900963\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "xlis = []\n",
    "ylis = []\n",
    "\n",
    "R=10\n",
    "size=1\n",
    "weights={}\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "batch={}\n",
    "\n",
    "W= np.random.uniform(-R,R,size=size)\n",
    "b= np.random.uniform(-R,R,size=size)\n",
    "b= random.choice(b)\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.random.uniform(-R,R,size=size)\n",
    "    y = np.random.normal(W*x+b,1,size=size)\n",
    "    xlis.append(x)\n",
    "    ylis.append(y)\n",
    "    flis.append(W*x+b)\n",
    "\n",
    "x=np.array(xlis)\n",
    "y=np.array(ylis)\n",
    "\n",
    "weights['W']=W\n",
    "weights['B']=b\n",
    "\n",
    "result=np.concatenate((x,y),axis=1)\n",
    "\n",
    "train_idx=int(result.shape[0]*0.85)\n",
    "dev_idx=int(result.shape[0]*0.05)\n",
    "test_idx=int(result.shape[0]*0.1)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "def linear_regression(data, idx, minibatch_size, epoch_size):\n",
    "    data_list=[]\n",
    "    X_batch= data[:,0]\n",
    "    y_batch= data[:,1]\n",
    "    X_batch=np.reshape(X_batch,(idx,size))\n",
    "    y_batch=np.reshape(y_batch,(idx,size))\n",
    "\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*************',j,'번차 epoch *************')\n",
    "        data=np.random.permutation(data)\n",
    "        X_batch= data[:,0]\n",
    "        y_batch= data[:,1]\n",
    "        X_batch=np.reshape(X_batch,(idx,size))\n",
    "        y_batch=np.reshape(y_batch,(idx,size))\n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            X_batch_temp=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y_temp=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            N=weights['W']*X_batch_temp\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y_temp-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch_temp\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y_temp # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.00001 * loss_grad[key]\n",
    "        \n",
    "        N=weights['W']*X_batch\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y_batch-f,2))\n",
    "        print('Loss',loss)\n",
    "      \n",
    "        #print('=================================')\n",
    "        \n",
    "        data_list.append(loss)\n",
    "        \n",
    "    X_batch = list(X_batch)\n",
    "    \n",
    "    #epoch에 따른 loss 출력도 가능\n",
    "    return plt.scatter(X_batch_temp, y_temp, label = 'name')\n",
    "\n",
    "print('Train_data')\n",
    "train_ = linear_regression(train_data_set,train_idx,50,30)\n",
    "\n",
    "#print('\\ndev_data')\n",
    "#dev_ = linear_regression(dev_data_set,dev_idx,20,30)\n",
    "\n",
    "print('\\nTest_data')\n",
    "Test_ = linear_regression(test_data_set,test_idx,50,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
