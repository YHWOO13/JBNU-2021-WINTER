{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 -2 -7]\n",
      " [-4  4 -4]]\n",
      "\n",
      " [[ 5 -4]\n",
      " [-2  4]\n",
      " [-7 -4]]\n",
      "\n",
      " [[ 0  2]\n",
      " [-4 -2]]\n",
      "\n",
      " [[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Linear regression을 위한 식 유도 (행렬에 대한 미분공식 이용)\n",
    "np.random.seed(0)\n",
    "w_row=random.randrange(2,4)\n",
    "w_col=random.randrange(2,4)\n",
    "x_col=random.randrange(2,4)\n",
    "\n",
    "w=np.random.randint(-7,7,(w_row,w_col))\n",
    "w_T=np.transpose(w)\n",
    "x=np.random.randint(-7,7,(w_row,x_col))\n",
    "bias=np.random.randint(0,1,(x_col,1))\n",
    "bias=np.ones_like(bias)\n",
    "\n",
    "print(w)\n",
    "print('\\n',w_T)\n",
    "print('\\n',x)\n",
    "print('\\n',bias)\n",
    "\n",
    "# 2.1-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.958]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[8.102]\n",
      " [9.014]\n",
      " [9.926]]\n",
      "f \n",
      " [[ 9.06 ]\n",
      " [ 9.972]\n",
      " [10.884]]\n",
      "Loss: 243.15384\n",
      "{'W': array([[272.808],\n",
      "       [107.664],\n",
      "       [161.496]]), 'B': array([53.832])}\n",
      "W\n",
      "before\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "after\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.958]]\n",
      "after\n",
      "[[0.904168]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[8.228528]\n",
      " [8.86772 ]\n",
      " [9.506912]]\n",
      "f \n",
      " [[ 9.132696]\n",
      " [ 9.771888]\n",
      " [10.41108 ]]\n",
      "Loss: 231.65519007936\n",
      "{'W': array([[423.607392],\n",
      "       [105.262656],\n",
      "       [157.893984]]), 'B': array([52.631328])}\n",
      "W\n",
      "before\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "after\n",
      "[[0.21558461]\n",
      " [0.70307334]\n",
      " [0.55461002]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.904168]]\n",
      "after\n",
      "[[0.85153667]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.64625676  42.99609865]\n",
      " [ -5.51605079 -26.7459619 ]\n",
      " [ -0.92076306  -1.5015161 ]\n",
      " [ -6.28035732 -32.52839932]\n",
      " [  7.47417393  48.00321634]\n",
      " [ -5.78214044 -27.50115255]\n",
      " [ -0.80774589   2.0454188 ]\n",
      " [ -4.3402399  -19.40489771]\n",
      " [  3.07703702  24.04524662]]\n",
      "[[ -4.3402399  -19.40489771]\n",
      " [ -5.51605079 -26.7459619 ]\n",
      " [ -6.28035732 -32.52839932]\n",
      " [  7.47417393  48.00321634]\n",
      " [ -0.92076306  -1.5015161 ]\n",
      " [  3.07703702  24.04524662]\n",
      " [  6.64625676  42.99609865]\n",
      " [ -0.80774589   2.0454188 ]\n",
      " [ -5.78214044 -27.50115255]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(R,size):\n",
    "    #size==특징의 개수\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(30):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result=data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)\n",
    "print(test_data_set)\n",
    "\n",
    "#print(result[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "미니배치 사이즈를 입력(10~100): 30\n",
      "epoch size(최대 100): 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bc7b0d616f49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch size(최대 100):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mminibatch_size\u001b[0m \u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mloss_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_x' is not defined"
     ]
    }
   ],
   "source": [
    "''' w,b를 찾아 가는 것. x와 y는 한세트라고 보면 된다\n",
    "    따라서, 셔플을 할 때 x와y로 이루어진 하나의 셋에 셔플을 주는 것이다.'''\n",
    "def data_set(R,size):\n",
    "  #  R=int(input('range: '))\n",
    " #   dimension_size=int(input('size: '))\n",
    "#    sample=int(input('sample: '))\n",
    "\n",
    "    xlis = []\n",
    "    ylis = []\n",
    "    wlis = []\n",
    "    \n",
    "    W= np.random.uniform(-R,R,size=size) #size==특징\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "    \n",
    "    mu=W*x+b\n",
    "    sigma=0.1*R\n",
    "\n",
    "    for i in range(10):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(mu,sigma,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "    \n",
    "x1=np.array(xlis)\n",
    "y1=np.array(ylis)\n",
    "\n",
    "'''\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')'''\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    \n",
    "    loss_grad={}    # dJdW, dJdB 저장공간\n",
    "    forward_info={} # 순방향 저장공간\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    loss(X_batch, y_batch, W, b)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    print(number_minibatch)\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "                \n",
    "                \n",
    "            print(loss)\n",
    "            print('=================================')\n",
    "             #print(loss_grad)\n",
    "\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        loss_gradient(train_data_x,train_data_y,train_data_W,train_data_b)\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "미니배치 사이즈를 입력(10~100): 50\n",
      "epoch size(최대 100): 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 49 열\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3486ded4c30d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_minibatch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'~'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'열'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mX_batch1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0my1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mW1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# 2.4.3 Linear regression 학습기 Practice\n",
    "\n",
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "b=np.reshape(b,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:860,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:860,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:860,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+train_data_b\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+dev_data_b\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+test_data_b\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=10)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(10,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b, minibatch_size, epoch_size):    \n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "\n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dJdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dJdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "\n",
    "    print(weights['W'])\n",
    "        #print(loss_grad)\n",
    "'''\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        \n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue\n",
    "'''\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    W=np.random.permutation(W)\n",
    "    b=np.random.permutation(b)\n",
    "    x=np.random.permutation(x)\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b,minibatch_size, epoch_size)\n",
    "        for key in weights.keys():\n",
    "\n",
    "            weights[key]=weights[key]- 0.01 * loss_grad[key]\n",
    "    print(weights['W'])\n",
    "    print('LOSS:',loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "미니배치 사이즈를 입력(10~100): 50\n",
      "epoch size(최대 100): 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev\n",
      "******* 1 번차 epoch*******\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2edc629322a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch size(최대 100):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dev'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mdev_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_data_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_data_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nTest'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[0mTest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2edc629322a1>\u001b[0m in \u001b[0;36mepoch\u001b[1;34m(x, y, W, b, minibatch_size, epoch_size)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mW1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mloss_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2edc629322a1>\u001b[0m in \u001b[0;36mloss_gradient\u001b[1;34m(X_batch, y_batch, W, b)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'B'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mchoice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "b1=random.choice(b)\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:900,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:900,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "'''\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:900,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "'''\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+b1\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+b1\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+b1\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=50)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(50,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "    \n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "    return loss_grad\n",
    "\n",
    "#loss(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "\n",
    "def epoch(x,y,W,b,minibatch_size,epoch_size):\n",
    "    number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "    weights = {'W':W,'B':b} \n",
    "    data_list=[]\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        W=np.random.permutation(W)\n",
    "        x=np.random.permutation(x)\n",
    "        b1=random.choice(b)\n",
    "\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            #print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            '''\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            '''\n",
    "            x1=x[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "           \n",
    "            loss_gradient(x1,y1,W1,b1)\n",
    "            \n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "        print('Loss:',loss(x,y,W,b)*0.00001)\n",
    "        data_list.append(loss(x,y,W,b)*0.00001)\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "print('Dev')\n",
    "dev_data=epoch(dev_data_x,dev_data_y,dev_data_W,b,minibatch_size,epoch_size)\n",
    "print('\\nTest')\n",
    "Test_data=epoch(test_data_x,test_data_y,test_data_W,test_data_b,minibatch_size,epoch_size)        \n",
    "print('\\nTrain')\n",
    "Train_data=epoch(train_data_x,train_data_y,train_data_W,train_data_b,minibatch_size,epoch_size)\n",
    "\n",
    "plt.plot(dev_data, label='dev_loss')\n",
    "plt.plot(Test_data, label='Test_loss')\n",
    "plt.plot(Train_data, label='Train_loss')\n",
    "plt.xlabel('Max_iter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
