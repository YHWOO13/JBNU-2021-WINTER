{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 MLP - Stochastic Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 3. 19. 2020 - \n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "  \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset 1번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset 2번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "mnist = datasets.fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mnist.data.shape, mnist.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "data=list(zip(mnist.data,mnist.target))\n",
    "#data=np.concatenate((mnist.data,mnist.target),axis=1)\n",
    "data=np.array(data)\n",
    "x=data[:,0]\n",
    "x=np.array(x)\n",
    "#y=data[:,1]\n",
    "print(x.shape)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "mnist.target = encoder.fit_transform(mnist.target.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "(59500, 784)\n",
      "(59500, 10)\n",
      "test\n",
      "(7000, 784)\n",
      "(7000, 10)\n",
      "dev\n",
      "(3500, 784)\n",
      "(3500, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_idx=int(data.shape[0]*0.85)\n",
    "dev_idx=int(data.shape[0]*0.05)\n",
    "test_idx=int(data.shape[0]*0.1)\n",
    "\n",
    "data=np.array(mnist.data)\n",
    "target=np.array(mnist.target)\n",
    "\n",
    "train_x=data[0:train_idx,:]\n",
    "test_x=data[train_idx:train_idx+test_idx,:]\n",
    "dev_x=data[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "\n",
    "train_y=target[0:train_idx,:]\n",
    "test_y=target[train_idx:train_idx+test_idx,:]\n",
    "dev_y=target[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print('train')\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print('test')\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print('dev')\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self,input_size,output_size):\n",
    "#         self.w = np.random.randn(input_size,output_size)\n",
    "#         self.b = np.random.randn(output_size)\n",
    "        \n",
    "        self.w = np. random.randn(input_size,output_size)\n",
    "        self.b = np.zeros(output_size)\n",
    "        self.dydw = None\n",
    "        self.dydb = 1 \n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.dydw = X.T\n",
    "        affine = np.dot(X,self.w) + self.b\n",
    "        \n",
    "        return affine\n",
    "    \n",
    "    def backward(self,dLdy,lr):\n",
    "        dLdw = np.dot(self.dydw,dLdy)\n",
    "        dLdb = np.sum(np.dot(dLdy, self.dydb),axis=0)\n",
    "        \n",
    "        dLdx = np.dot(dLdy,np.transpose(self.w))\n",
    "       \n",
    "        self.w -= lr * dLdw\n",
    "        \n",
    "        self.b -= lr * dLdb\n",
    "        \n",
    "        return dLdx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.30865396 -1.35450396 -2.62433206 -0.71365799  0.27258047]\n",
      " [ 2.01329669 -0.52990518  0.38160162 -0.57185455 -0.4313999 ]\n",
      " [-1.07405709 -0.94668426  0.69859986 -0.23478413 -1.02281122]\n",
      " [ 1.82819344  0.61929077  0.59288127 -0.52661734  0.28780851]]\n",
      "[[ 1.15865396e+00 -1.53450396e+00 -2.83433206e+00 -9.53657989e-01\n",
      "   2.58046905e-03]\n",
      " [ 1.71329669e+00 -8.89905177e-01 -3.83983786e-02 -1.05185455e+00\n",
      "  -9.71399901e-01]\n",
      " [-1.52405709e+00 -1.48668426e+00  6.85998578e-02 -9.54784129e-01\n",
      "  -1.83281122e+00]\n",
      " [ 1.22819344e+00 -1.00709229e-01 -2.47118728e-01 -1.48661734e+00\n",
      "  -7.92191490e-01]]\n"
     ]
    }
   ],
   "source": [
    "l=Layer(4,5)\n",
    "\n",
    "l.forward(np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]))\n",
    "print(l.w)\n",
    "l.backward(np.arange(15).reshape(3,5),0.01)\n",
    "print(l.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Relu class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "#         print('dout',dout.shape)\n",
    "#         print('mask',self.mask.shape)\n",
    "        dout[self.mask]= 0\n",
    "        #dout * np.where(self.mask==True,1,0)\n",
    "        return dout * np.where(self.mask==True,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막\n",
    "class Model():\n",
    "    def __init__(self,length):\n",
    "        self.layers = [Layer(784,100),\n",
    "                       Relu(),\n",
    "                       Layer(100,50),\n",
    "                       Relu(),\n",
    "                       Layer(50,30),\n",
    "                       Relu(),\n",
    "                       Layer(30,10),\n",
    "                       ]\n",
    "    \n",
    "    def forward(self,put):\n",
    "        output = self.layers[0].forward(put)\n",
    "        output = self.layers[1].forward(output)\n",
    "        \n",
    "        output = self.layers[2].forward(output)\n",
    "        output = self.layers[3].forward(output)\n",
    "        \n",
    "        output = self.layers[4].forward(output)\n",
    "        output = self.layers[5].forward(output)\n",
    "        \n",
    "        output = self.layers[6].forward(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dld_,lr):\n",
    "        dld_ = self.layers[6].backward(dld_,lr)\n",
    "        dld_ = self.layers[5].backward(dld_)\n",
    "        dld_ = self.layers[4].backward(dld_,lr)\n",
    "        dld_ = self.layers[3].backward(dld_)\n",
    "        dld_ = self.layers[2].backward(dld_,lr)\n",
    "        dld_ = self.layers[1].backward(dld_)\n",
    "        dld_ = self.layers[0].backward(dld_,lr)\n",
    "        return dld_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make softmax class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Softmax():    \n",
    "    def __init__(self):\n",
    "        self.exp_o = 0\n",
    "        self.sum_exp_o = 0\n",
    "        self.delta = 0\n",
    "        \n",
    "    def forward(self,h):\n",
    "        self.delta = np.max(h)\n",
    "        \n",
    "        self.exp_o = np.exp(h - self.delta)\n",
    "        self.sum_exp_o = np.sum(self.exp_o)\n",
    "        self.softmax = self.exp_o / self.sum_exp_o\n",
    "        \n",
    "        return self.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Cross_entropy class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CE():\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        \n",
    "    def forward(self, softMAX, y):\n",
    "        # cross_entropy + 마이너스 무한대가 생기지 않도록 만듦\n",
    "        delta = 1e-13\n",
    "        \n",
    "        return - np.sum(y * np.log(softMAX + delta))\n",
    "        \n",
    "    def backward(self, y, softMAX):\n",
    "        batch_size = y.shape[0]\n",
    "        dJdR = (softMAX-y) / batch_size\n",
    "        return dJdR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax - with - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxwithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def softmax(self,a):\n",
    "        c = np.max(a,axis=-1,keepdims=True)\n",
    "        exp_a = np.exp(a-c)\n",
    "        sum_exp_a = np.sum(exp_a,axis=-1,keepdims=True)\n",
    "        y = exp_a / sum_exp_a\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def cross_entropy_error(self,y,t):\n",
    "        delta = 1e-13\n",
    "        return -np.sum(t * np.log(y + delta)) / x.shape[0]\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]\n",
      " [8]]\n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "print(np.max(a,axis=-1,keepdims=True))\n",
    "\n",
    "sml = SoftmaxwithLoss()\n",
    "b = sml.softmax(a)\n",
    "print(b)\n",
    "print(np.sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Minibatch size(10~500):  100\n",
      "epoch size:  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== train ==============================\n",
      "************* 10 번차 epoch *************\n",
      "정확도\n",
      "0.36031932773109243\n",
      "************* 20 번차 epoch *************\n",
      "정확도\n",
      "0.4523697478991597\n",
      "************* 30 번차 epoch *************\n",
      "정확도\n",
      "0.4844033613445378\n",
      "************* 40 번차 epoch *************\n",
      "정확도\n",
      "0.5412436974789916\n",
      "************* 50 번차 epoch *************\n",
      "정확도\n",
      "0.5112941176470588\n",
      "************* 60 번차 epoch *************\n",
      "정확도\n",
      "0.5196470588235295\n",
      "************* 70 번차 epoch *************\n",
      "정확도\n",
      "0.5395294117647059\n"
     ]
    }
   ],
   "source": [
    "model = Model(3)\n",
    "relu = Relu()\n",
    "swl = SoftmaxwithLoss()\n",
    "\n",
    "length = 3\n",
    "minibatch_size = int(input('Minibatch size(10~500): '))\n",
    "epoch_size = int(input('epoch size: '))\n",
    "iter_per_epoch = 10\n",
    "test_acc_list = []\n",
    "\n",
    "# td = np.array([[1,2,3], [1,0,0], [3,2,3], [3,0,0], [5,2,3], [5,0,0], [7,2,3], [7,0,0], [9,2,3]])\n",
    "# tg = np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9]])\n",
    "\n",
    "\n",
    "def accuracy(x, t):\n",
    "    acc_y = model.forward(x)\n",
    "    acc_y = swl.softmax(acc_y)\n",
    "    acc_y = np.argmax(acc_y, axis=1)\n",
    "    if t.ndim != 1 : t = np.argmax(t,axis=1)\n",
    "        \n",
    "    accuracy = np.sum(acc_y == t) / float(x.shape[0])\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def learning(X,y, minibatch_size, epoch_size, learning_rate):\n",
    "    acc_list = []\n",
    "    for e in range(1,epoch_size+1):\n",
    "        \n",
    "        combined = list(zip(X, y))\n",
    "        permut = np.random.permutation(combined)\n",
    "        X[:], y[:] = zip(*permut)\n",
    "        #X = X/255\n",
    "    \n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X.shape[0] / minibatch_size))\n",
    "        \n",
    "        for n in range(1, number_minibatch+1):\n",
    "\n",
    "            X_temp=X[minibatch_size * n-(minibatch_size-1)-1:minibatch_size*n]\n",
    "            y_temp=y[minibatch_size * n-(minibatch_size-1)-1:minibatch_size*n]\n",
    "            \n",
    "            c = model.forward(X_temp)\n",
    "            J = swl.forward(c,y_temp)\n",
    "        \n",
    "            dJdh = swl.backward()\n",
    "            model.backward(dJdh,learning_rate)\n",
    "\n",
    " \n",
    "        c = model.forward(X)\n",
    "        J = swl.forward(c,y)\n",
    "\n",
    "\n",
    "        acc = accuracy(X, y)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        if e % iter_per_epoch == 0:\n",
    "            print('*************',e,'번차 epoch *************')\n",
    "    #             print('최종')\n",
    "    #             print(J)\n",
    "            print('정확도')\n",
    "            print(acc)\n",
    "        \n",
    "    return acc_list\n",
    "\n",
    "#print(learning(train_x, train_y, minibatch_size, epoch_size, 0.01))\n",
    "print('='*30,'train','='*30)\n",
    "train = learning(train_x, train_y, minibatch_size, epoch_size, 0.0000001)\n",
    "print('='*30,'test','='*30)\n",
    "test = learning(test_x, test_y, minibatch_size, epoch_size, 0.0000001)\n",
    "print('='*30,'dev','='*30)\n",
    "dev = learning(dev_x, dev_y, minibatch_size, epoch_size, 0.0000001)\n",
    " \n",
    "\n",
    "x = np.arange(len(train))\n",
    "print('x_length', x)\n",
    "plt.plot(x, train, label='train acc')\n",
    "plt.plot(x, test, label='test acc', linestyle='--')\n",
    "plt.plot(x, dev, label='dev acc', linestyle='-.')\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "# print(\"train acc, test acc | \" \n",
    "#       + str(learning(x_train, t_train, minibatch_size, epoch_size, 0.0001)) + \", \" \n",
    "#       + str(learning(x_test, t_test, minibatch_size, epoch_size, 0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
