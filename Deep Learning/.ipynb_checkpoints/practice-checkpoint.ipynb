{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 6]\n",
      "[[1 0 0]\n",
      " [0 4 0]\n",
      " [0 0 6]]\n",
      "11\n",
      "[[ 1  4 49]\n",
      " [ 9 16 64]\n",
      " [25 81 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "a=[[1,2,7],[3,4,8],[5,9,6]]\n",
    "a=np.array(a)\n",
    "c=np.transpose(a)\n",
    "b=np.diag(a)\n",
    "e=np.diag(np.diag(a))\n",
    "print(b)\n",
    "print(e)\n",
    "d=np.trace(e)\n",
    "print(d)\n",
    "\n",
    "g=np.power(a,2)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4, 49],\n",
       "       [ 9, 16, 64],\n",
       "       [25, 81, 36]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return np.power(x,2)\n",
    "square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_deriv(chain, input_range):\n",
    "    \n",
    "    assert len(chain)==2\n",
    "    f1=Chain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "a=13\n",
    "for i in range(1,a+1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(3).reshape(3,1)\n",
    "print(a)\n",
    "a+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 241.66666666666666\n",
      "\n",
      " {'X': array([[1, 2, 7],\n",
      "       [3, 4, 8],\n",
      "       [5, 9, 6]]), 'N': array([[10],\n",
      "       [15],\n",
      "       [20]]), 'f': array([[11],\n",
      "       [16],\n",
      "       [21]]), 'y': array([[1],\n",
      "       [1],\n",
      "       [1]])}\n",
      "B: [[1]]\n",
      "1 W\n",
      "2 B\n"
     ]
    }
   ],
   "source": [
    "# 2.3.4 코드로 살펴보는 선형회귀linear regression\n",
    "\n",
    "matrix= [[1,2,7],[3,4,8],[5,9,6]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "def forward_linear_Regression(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info={}\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N\n",
    "    forward_info['f']= f\n",
    "    forward_info['y']= y_batch\n",
    "    \n",
    "    return print('loss', loss), print('\\n', forward_info)\n",
    "    \n",
    "\n",
    "forward_linear_Regression(matrix,y,weights)\n",
    "\n",
    "print('B:',B)\n",
    "\n",
    "o=1\n",
    "for key in weights.keys():\n",
    "    print(o,key)\n",
    "    o=o+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N \n",
      " [[10]\n",
      " [15]\n",
      " [20]]\n",
      "f \n",
      " [[11]\n",
      " [16]\n",
      " [21]]\n",
      "725\n",
      "\n",
      " dJdf\n",
      "[[20]\n",
      " [30]\n",
      " [40]]\n",
      "\n",
      " dfdN\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "\n",
      " dNdW\n",
      "[[1 3 5]\n",
      " [2 4 9]\n",
      " [7 8 6]]\n",
      "\n",
      " dJdN=dJdf*dfdN\n",
      "[[20]\n",
      " [30]\n",
      " [40]]\n",
      "==================================================================\n",
      "batch gradient descent\n",
      "\n",
      "\n",
      "1 회 반복\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.69]\n",
      " [0.48]\n",
      " [0.38]]\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.91]]\n",
      "\n",
      "\n",
      "2 회 반복\n",
      "before\n",
      "[[0.69]\n",
      " [0.48]\n",
      " [0.38]]\n",
      "after\n",
      "[[ 0.38]\n",
      " [-0.04]\n",
      " [-0.24]]\n",
      "before\n",
      "[[0.91]]\n",
      "after\n",
      "[[0.82]]\n",
      "\n",
      "\n",
      "3 회 반복\n",
      "before\n",
      "[[ 0.38]\n",
      " [-0.04]\n",
      " [-0.24]]\n",
      "after\n",
      "[[ 0.07]\n",
      " [-0.56]\n",
      " [-0.86]]\n",
      "before\n",
      "[[0.82]]\n",
      "after\n",
      "[[0.73]]\n"
     ]
    }
   ],
   "source": [
    "# 2.4.3 전체코드로 본 도함수 계산과정\n",
    "\n",
    "matrix= [[1,2,7],[3,4,8],[5,9,6]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print(loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 2.4.3 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    '''# ??=>이건 왜 구하는 거임? 48번째줄을 구할때 dot으로 여태까지 계산을 위해서인가...\n",
    "    \n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    print('\\n dJdf')\n",
    "    print(dJdf)\n",
    "    \n",
    "    print('\\n dfdN')\n",
    "    print(dfdN)\n",
    "    \n",
    "    print('\\n dNdW')\n",
    "    print(dNdW)\n",
    "    \n",
    "    print('\\n dJdN=dJdf*dfdN')\n",
    "    print(dJdN)\n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return \n",
    "\n",
    "loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "batch(loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.958]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[8.102]\n",
      " [9.014]\n",
      " [9.926]]\n",
      "f \n",
      " [[ 9.06 ]\n",
      " [ 9.972]\n",
      " [10.884]]\n",
      "Loss: 243.15384\n",
      "{'W': array([[272.808],\n",
      "       [107.664],\n",
      "       [161.496]]), 'B': array([53.832])}\n",
      "W\n",
      "before\n",
      "[[0.912]\n",
      " [0.916]\n",
      " [0.874]]\n",
      "after\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.958]]\n",
      "after\n",
      "[[0.904168]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[8.228528]\n",
      " [8.86772 ]\n",
      " [9.506912]]\n",
      "f \n",
      " [[ 9.132696]\n",
      " [ 9.771888]\n",
      " [10.41108 ]]\n",
      "Loss: 231.65519007936\n",
      "{'W': array([[423.607392],\n",
      "       [105.262656],\n",
      "       [157.893984]]), 'B': array([52.631328])}\n",
      "W\n",
      "before\n",
      "[[0.639192]\n",
      " [0.808336]\n",
      " [0.712504]]\n",
      "after\n",
      "[[0.21558461]\n",
      " [0.70307334]\n",
      " [0.55461002]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.904168]]\n",
      "after\n",
      "[[0.85153667]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "# 미니배치\n",
    "\n",
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    '''# ??=>이건 왜 구하는 거임? 48번째줄을 구할때 dot으로 여태까지 계산을 위해서인가...\n",
    "    \n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\\nreal data에서 모델의 성능이 낮아지는 경향을 막기 위해서 \\n\\n1. 데이터를 training data, validation data, test data로 나눈다\\n2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것\\n3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다)\\n4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early stopping 추가한 Algorithm of mini-batch SGD method\n",
    "'''\n",
    "에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\n",
    "real data에서 모델의 성능이 낮아지는 경향을 막기 위해서 \n",
    "\n",
    "1. 데이터를 training data, validation data, test data로 나눈다\n",
    "2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것\n",
    "3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다)\n",
    "4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.9958]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.9102]\n",
      " [ 9.9014]\n",
      " [10.8926]]\n",
      "f \n",
      " [[ 9.906 ]\n",
      " [10.8972]\n",
      " [11.8884]]\n",
      "Loss: 295.8286584\n",
      "{'W': array([[300.8808],\n",
      "       [118.7664],\n",
      "       [178.1496]]), 'B': array([59.3832])}\n",
      "W\n",
      "before\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "after\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9958]]\n",
      "after\n",
      "[[0.98986168]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[11.59598528]\n",
      " [12.5570972 ]\n",
      " [13.51820912]]\n",
      "f \n",
      " [[12.58584696]\n",
      " [13.54695888]\n",
      " [14.5080708 ]]\n",
      "Loss: 474.1260036547649\n",
      "{'W': array([[606.09847392],\n",
      "       [150.56350656],\n",
      "       [225.84525984]]), 'B': array([75.28175328])}\n",
      "W\n",
      "before\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "after\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.98986168]]\n",
      "after\n",
      "[[0.9823335]]\n",
      "\n",
      "\n",
      "******* 2 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.67083763]\n",
      " [6.57133971]\n",
      " [7.47184178]]\n",
      "f \n",
      " [[6.65317114]\n",
      " [7.55367321]\n",
      " [8.45417528]]\n",
      "Loss: 130.47370562049468\n",
      "{'W': array([[ 82.24608682],\n",
      "       [ 78.64407853],\n",
      "       [117.96611779]]), 'B': array([39.32203926])}\n",
      "W\n",
      "before\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "after\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9823335]]\n",
      "after\n",
      "[[0.9784013]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.28832677]\n",
      " [ 9.18060423]\n",
      " [10.07288169]]\n",
      "f \n",
      " [[ 9.26672807]\n",
      " [10.15900553]\n",
      " [11.05128299]]\n",
      "Loss: 253.25446504480863\n",
      "{'W': array([[278.33927576],\n",
      "       [109.90806636],\n",
      "       [164.86209954]]), 'B': array([54.95403318])}\n",
      "W\n",
      "before\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "after\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9784013]]\n",
      "after\n",
      "[[0.9729059]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[10.69888142]\n",
      " [11.56332496]\n",
      " [12.42776849]]\n",
      "f \n",
      " [[11.67178732]\n",
      " [12.53623085]\n",
      " [13.40067439]]\n",
      "Loss: 400.74839226007464\n",
      "{'W': array([[557.19685518],\n",
      "       [138.43477026],\n",
      "       [207.65215539]]), 'B': array([69.21738513])}\n",
      "W\n",
      "before\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "after\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9729059]]\n",
      "after\n",
      "[[0.96598416]]\n",
      "\n",
      "\n",
      "******* 3 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.36651792]\n",
      " [6.17524177]\n",
      " [6.98396562]]\n",
      "f \n",
      " [[6.33250208]\n",
      " [7.14122593]\n",
      " [7.94994978]]\n",
      "Loss: 114.45203617371786\n",
      "{'W': array([[ 76.92960652],\n",
      "       [ 73.69471112],\n",
      "       [110.54206668]]), 'B': array([36.84735556])}\n",
      "W\n",
      "before\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "after\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96598416]]\n",
      "after\n",
      "[[0.96229942]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.71401606]\n",
      " [8.51504695]\n",
      " [9.31607784]]\n",
      "f \n",
      " [[ 8.67631549]\n",
      " [ 9.47734638]\n",
      " [10.27837727]]\n",
      "Loss: 216.87950580813276\n",
      "{'W': array([[257.5245149 ],\n",
      "       [101.72815654],\n",
      "       [152.5922348 ]]), 'B': array([50.86407827])}\n",
      "W\n",
      "before\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "after\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96229942]]\n",
      "after\n",
      "[[0.95721302]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.87071827]\n",
      " [10.64599671]\n",
      " [11.42127515]]\n",
      "f \n",
      " [[10.82793129]\n",
      " [11.60320973]\n",
      " [12.37848817]]\n",
      "Loss: 338.4862828577476\n",
      "{'W': array([[512.05518065],\n",
      "       [127.23851672],\n",
      "       [190.85777509]]), 'B': array([63.61925836])}\n",
      "W\n",
      "before\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "after\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95721302]]\n",
      "after\n",
      "[[0.95085109]]\n",
      "\n",
      "\n",
      "******* 4 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.08513709]\n",
      " [5.80921001]\n",
      " [6.53328293]]\n",
      "f \n",
      " [[6.03598818]\n",
      " [6.7600611 ]\n",
      " [7.48413402]]\n",
      "Loss: 100.58347471209066\n",
      "{'W': array([[ 72.01702485],\n",
      "       [ 69.12073316],\n",
      "       [103.68109975]]), 'B': array([34.56036658])}\n",
      "W\n",
      "before\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "after\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95085109]]\n",
      "after\n",
      "[[0.94739505]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.18362056]\n",
      " [7.90049178]\n",
      " [8.617363  ]]\n",
      "f \n",
      " [[8.13101562]\n",
      " [8.84788683]\n",
      " [9.56475805]]\n",
      "Loss: 185.7957919285563\n",
      "{'W': array([[238.30408987],\n",
      "       [ 94.174642  ],\n",
      "       [141.261963  ]]), 'B': array([47.087321])}\n",
      "W\n",
      "before\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "after\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94739505]]\n",
      "after\n",
      "[[0.94268632]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.10620784]\n",
      " [ 9.79924865]\n",
      " [10.49228945]]\n",
      "f \n",
      " [[10.04889416]\n",
      " [10.74193497]\n",
      " [11.43497578]]\n",
      "Loss: 285.6765017928727\n",
      "{'W': array([[470.38504162],\n",
      "       [116.9032196 ],\n",
      "       [175.35482939]]), 'B': array([58.4516098])}\n",
      "W\n",
      "before\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "after\n",
      "[[0.6460023 ]\n",
      " [0.87368232]\n",
      " [0.81052348]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94268632]]\n",
      "after\n",
      "[[0.93684116]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    '''# ??=>이건 왜 구하는 거임? 48번째줄을 구할때 dot으로 여태까지 계산을 위해서인가...\n",
    "    \n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "epoch_size=4\n",
    "\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        loss_gradient(matrix1,y1,weights)\n",
    "\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.45038264 12.29956889 11.50821893 11.90313074 12.94222382 12.91425424\n",
      " 12.08819369 11.66157355 12.71078975 13.12802859 14.21040731 12.64245272\n",
      " 13.19138508 12.2510898  13.44691282 11.70429944 13.14744444 13.14206605\n",
      " 12.02838634 12.75434309 12.55761989 14.82320934 14.30207233 13.06557033\n",
      " 14.34622669 12.66406696 14.34540979 13.71469583 14.13378531 12.28141575\n",
      " 13.9157226  13.39607021 11.55733146 15.53606321 14.35805429 11.74301846\n",
      " 12.39424499 12.53915693 11.38122172 13.35854021 14.78683754 11.80789035\n",
      " 11.85620677 12.76530064 13.9280876  13.93283471 13.2114067  13.30359584\n",
      " 12.77432468 11.98749567 13.04507182 14.18510423 13.17954839 13.33300947\n",
      " 11.62800681 12.6425882  12.30360583 12.33947817 11.66523802 14.55895034\n",
      " 14.50869783 13.40229863 13.27404702 11.04143825 14.0709551  13.11577299\n",
      " 13.30296937 11.63147036 13.44014894 12.08086465 13.00296332 12.77218098\n",
      " 13.63022055 13.28485422 14.15348068 13.17890048 12.67196242 12.75738063\n",
      " 11.51942692 12.65756752 12.50091949 12.53127898 14.03652158 12.91507488\n",
      " 13.68360419 14.17007535 12.59312977 13.17344263 14.48530087 13.10190469\n",
      " 13.7102965  14.47507309 12.12444855 12.68245695 14.39785814 12.74244016\n",
      " 12.18818857 12.19520136 12.65947973 12.21483884]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 2., 0., 2., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 2., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 2., 0., 1.,\n",
       "        0., 1., 0., 0., 2., 2., 2., 1., 0., 1., 0., 1., 2., 3., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 2., 0., 3., 1., 1., 0., 0., 0., 0., 2., 2., 0., 1.,\n",
       "        0., 1., 0., 0., 2., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 2.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 2., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([11.04143825, 11.05642033, 11.07140241, 11.08638449, 11.10136658,\n",
       "        11.11634866, 11.13133074, 11.14631283, 11.16129491, 11.17627699,\n",
       "        11.19125908, 11.20624116, 11.22122324, 11.23620533, 11.25118741,\n",
       "        11.26616949, 11.28115158, 11.29613366, 11.31111574, 11.32609783,\n",
       "        11.34107991, 11.35606199, 11.37104408, 11.38602616, 11.40100824,\n",
       "        11.41599033, 11.43097241, 11.44595449, 11.46093658, 11.47591866,\n",
       "        11.49090074, 11.50588283, 11.52086491, 11.53584699, 11.55082908,\n",
       "        11.56581116, 11.58079324, 11.59577532, 11.61075741, 11.62573949,\n",
       "        11.64072157, 11.65570366, 11.67068574, 11.68566782, 11.70064991,\n",
       "        11.71563199, 11.73061407, 11.74559616, 11.76057824, 11.77556032,\n",
       "        11.79054241, 11.80552449, 11.82050657, 11.83548866, 11.85047074,\n",
       "        11.86545282, 11.88043491, 11.89541699, 11.91039907, 11.92538116,\n",
       "        11.94036324, 11.95534532, 11.97032741, 11.98530949, 12.00029157,\n",
       "        12.01527366, 12.03025574, 12.04523782, 12.06021991, 12.07520199,\n",
       "        12.09018407, 12.10516615, 12.12014824, 12.13513032, 12.1501124 ,\n",
       "        12.16509449, 12.18007657, 12.19505865, 12.21004074, 12.22502282,\n",
       "        12.2400049 , 12.25498699, 12.26996907, 12.28495115, 12.29993324,\n",
       "        12.31491532, 12.3298974 , 12.34487949, 12.35986157, 12.37484365,\n",
       "        12.38982574, 12.40480782, 12.4197899 , 12.43477199, 12.44975407,\n",
       "        12.46473615, 12.47971824, 12.49470032, 12.5096824 , 12.52466449,\n",
       "        12.53964657, 12.55462865, 12.56961073, 12.58459282, 12.5995749 ,\n",
       "        12.61455698, 12.62953907, 12.64452115, 12.65950323, 12.67448532,\n",
       "        12.6894674 , 12.70444948, 12.71943157, 12.73441365, 12.74939573,\n",
       "        12.76437782, 12.7793599 , 12.79434198, 12.80932407, 12.82430615,\n",
       "        12.83928823, 12.85427032, 12.8692524 , 12.88423448, 12.89921657,\n",
       "        12.91419865, 12.92918073, 12.94416282, 12.9591449 , 12.97412698,\n",
       "        12.98910907, 13.00409115, 13.01907323, 13.03405532, 13.0490374 ,\n",
       "        13.06401948, 13.07900156, 13.09398365, 13.10896573, 13.12394781,\n",
       "        13.1389299 , 13.15391198, 13.16889406, 13.18387615, 13.19885823,\n",
       "        13.21384031, 13.2288224 , 13.24380448, 13.25878656, 13.27376865,\n",
       "        13.28875073, 13.30373281, 13.3187149 , 13.33369698, 13.34867906,\n",
       "        13.36366115, 13.37864323, 13.39362531, 13.4086074 , 13.42358948,\n",
       "        13.43857156, 13.45355365, 13.46853573, 13.48351781, 13.4984999 ,\n",
       "        13.51348198, 13.52846406, 13.54344614, 13.55842823, 13.57341031,\n",
       "        13.58839239, 13.60337448, 13.61835656, 13.63333864, 13.64832073,\n",
       "        13.66330281, 13.67828489, 13.69326698, 13.70824906, 13.72323114,\n",
       "        13.73821323, 13.75319531, 13.76817739, 13.78315948, 13.79814156,\n",
       "        13.81312364, 13.82810573, 13.84308781, 13.85806989, 13.87305198,\n",
       "        13.88803406, 13.90301614, 13.91799823, 13.93298031, 13.94796239,\n",
       "        13.96294448, 13.97792656, 13.99290864, 14.00789073, 14.02287281,\n",
       "        14.03785489, 14.05283697, 14.06781906, 14.08280114, 14.09778322,\n",
       "        14.11276531, 14.12774739, 14.14272947, 14.15771156, 14.17269364,\n",
       "        14.18767572, 14.20265781, 14.21763989, 14.23262197, 14.24760406,\n",
       "        14.26258614, 14.27756822, 14.29255031, 14.30753239, 14.32251447,\n",
       "        14.33749656, 14.35247864, 14.36746072, 14.38244281, 14.39742489,\n",
       "        14.41240697, 14.42738906, 14.44237114, 14.45735322, 14.47233531,\n",
       "        14.48731739, 14.50229947, 14.51728156, 14.53226364, 14.54724572,\n",
       "        14.5622278 , 14.57720989, 14.59219197, 14.60717405, 14.62215614,\n",
       "        14.63713822, 14.6521203 , 14.66710239, 14.68208447, 14.69706655,\n",
       "        14.71204864, 14.72703072, 14.7420128 , 14.75699489, 14.77197697,\n",
       "        14.78695905, 14.80194114, 14.81692322, 14.8319053 , 14.84688739,\n",
       "        14.86186947, 14.87685155, 14.89183364, 14.90681572, 14.9217978 ,\n",
       "        14.93677989, 14.95176197, 14.96674405, 14.98172614, 14.99670822,\n",
       "        15.0116903 , 15.02667238, 15.04165447, 15.05663655, 15.07161863,\n",
       "        15.08660072, 15.1015828 , 15.11656488, 15.13154697, 15.14652905,\n",
       "        15.16151113, 15.17649322, 15.1914753 , 15.20645738, 15.22143947,\n",
       "        15.23642155, 15.25140363, 15.26638572, 15.2813678 , 15.29634988,\n",
       "        15.31133197, 15.32631405, 15.34129613, 15.35627822, 15.3712603 ,\n",
       "        15.38624238, 15.40122447, 15.41620655, 15.43118863, 15.44617072,\n",
       "        15.4611528 , 15.47613488, 15.49111697, 15.50609905, 15.52108113,\n",
       "        15.53606321]),\n",
       " <a list of 300 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=np.random.normal(13,1,size=100)\n",
    "print(a)\n",
    "plt.hist(a,bins=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.36079045  6.72220472 -6.51807328 -9.65834185 -9.89048105  5.27212167\n",
      "  9.74473263  7.39164681 -3.55242025 -6.32168391]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 0., 2., 0., 0., 0., 1., 2., 1.]),\n",
       " array([-9.89048105, -7.92695969, -5.96343832, -3.99991695, -2.03639558,\n",
       "        -0.07287421,  1.89064715,  3.85416852,  5.81768989,  7.78121126,\n",
       "         9.74473263]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b=np.random.uniform(-10,10,size=10)\n",
    "print(b)\n",
    "plt.hist(b,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "b=np.reshape(b,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "\n",
    "mu=np.dot(np.transpose(W),x)+b\n",
    "sigma=0.1*10\n",
    "\n",
    "y=np.random.normal(mu,sigma,size=1000)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* 1 번차 epoch*******\n",
      "result\n",
      "[[  4.12750817  32.94168911]\n",
      " [ -4.2304668  -22.56309671]\n",
      " [ -4.8876617  -26.49924158]\n",
      " [  5.14709099  38.10479831]\n",
      " [  4.45837063  36.64589935]\n",
      " [ -4.24738607 -22.79903354]\n",
      " [  8.22424253  62.20972663]\n",
      " [ -1.9113712   -7.02103244]\n",
      " [  3.00619632  25.47510763]\n",
      " [ -4.47342113 -25.11789023]]\n",
      "{'W': array([6.61695677]), 'B': 5.7406271718374935}\n",
      "\n",
      " 0 ~ 2 열\n",
      "[[ 5.14709099]\n",
      " [ 4.12750817]\n",
      " [-4.24738607]]\n",
      "[[ 38.10479831]\n",
      " [ 32.94168911]\n",
      " [-22.79903354]]\n",
      "before\n",
      "{'W': array([6.61695677]), 'B': 5.7406271718374935}\n",
      "after\n",
      "{'W': array([[6.47040559]]), 'B': array([5.69584159])}\n",
      "\n",
      " 3 ~ 5 열\n",
      "[[ 8.22424253]\n",
      " [ 4.45837063]\n",
      " [-1.9113712 ]]\n",
      "[[62.20972663]\n",
      " [36.64589935]\n",
      " [-7.02103244]]\n",
      "before\n",
      "{'W': array([[6.47040559]]), 'B': array([5.69584159])}\n",
      "after\n",
      "{'W': array([[7.21400045]]), 'B': array([5.79689688])}\n",
      "\n",
      " 6 ~ 8 열\n",
      "[[-4.2304668 ]\n",
      " [ 3.00619632]]\n",
      "[[-22.56309671]\n",
      " [ 25.47510763]]\n",
      "before\n",
      "{'W': array([[7.21400045]]), 'B': array([5.79689688])}\n",
      "after\n",
      "{'W': array([[6.91060473]]), 'B': array([5.79989898])}\n",
      "Loss 0.9846375385970318\n",
      "=================================\n",
      "******* 2 번차 epoch*******\n",
      "result\n",
      "[[ -4.8876617  -26.49924158]\n",
      " [ -4.24738607 -22.79903354]\n",
      " [  4.45837063  36.64589935]\n",
      " [ -4.47342113 -25.11789023]\n",
      " [  5.14709099  38.10479831]\n",
      " [  3.00619632  25.47510763]\n",
      " [ -4.2304668  -22.56309671]\n",
      " [  8.22424253  62.20972663]\n",
      " [ -1.9113712   -7.02103244]\n",
      " [  4.12750817  32.94168911]]\n",
      "{'W': array([[6.91060473]]), 'B': array([5.79989898])}\n",
      "\n",
      " 0 ~ 2 열\n",
      "[[ 5.14709099]\n",
      " [ 4.12750817]\n",
      " [-4.24738607]]\n",
      "[[ 38.10479831]\n",
      " [ 32.94168911]\n",
      " [-22.79903354]]\n",
      "before\n",
      "{'W': array([[6.91060473]]), 'B': array([5.79989898])}\n",
      "after\n",
      "{'W': array([[6.39650096]]), 'B': array([5.72203246])}\n",
      "\n",
      " 3 ~ 5 열\n",
      "[[ 8.22424253]\n",
      " [ 4.45837063]\n",
      " [-1.9113712 ]]\n",
      "[[62.20972663]\n",
      " [36.64589935]\n",
      " [-7.02103244]]\n",
      "before\n",
      "{'W': array([[6.39650096]]), 'B': array([5.72203246])}\n",
      "after\n",
      "{'W': array([[7.26920925]]), 'B': array([5.83743719])}\n",
      "\n",
      " 6 ~ 8 열\n",
      "[[-4.2304668 ]\n",
      " [ 3.00619632]]\n",
      "[[-22.56309671]\n",
      " [ 25.47510763]]\n",
      "before\n",
      "{'W': array([[7.26920925]]), 'B': array([5.83743719])}\n",
      "after\n",
      "{'W': array([[6.93706623]]), 'B': array([5.84016949])}\n",
      "Loss 1.1886225163056032\n",
      "=================================\n",
      "******* 3 번차 epoch*******\n",
      "result\n",
      "[[ -4.47342113 -25.11789023]\n",
      " [  5.14709099  38.10479831]\n",
      " [ -4.2304668  -22.56309671]\n",
      " [  8.22424253  62.20972663]\n",
      " [  3.00619632  25.47510763]\n",
      " [  4.45837063  36.64589935]\n",
      " [ -1.9113712   -7.02103244]\n",
      " [  4.12750817  32.94168911]\n",
      " [ -4.8876617  -26.49924158]\n",
      " [ -4.24738607 -22.79903354]]\n",
      "{'W': array([[6.93706623]]), 'B': array([5.84016949])}\n",
      "\n",
      " 0 ~ 2 열\n",
      "[[ 5.14709099]\n",
      " [ 4.12750817]\n",
      " [-4.24738607]]\n",
      "[[ 38.10479831]\n",
      " [ 32.94168911]\n",
      " [-22.79903354]]\n",
      "before\n",
      "{'W': array([[6.93706623]]), 'B': array([5.84016949])}\n",
      "after\n",
      "{'W': array([[6.38632924]]), 'B': array([5.7572262])}\n",
      "\n",
      " 3 ~ 5 열\n",
      "[[ 8.22424253]\n",
      " [ 4.45837063]\n",
      " [-1.9113712 ]]\n",
      "[[62.20972663]\n",
      " [36.64589935]\n",
      " [-7.02103244]]\n",
      "before\n",
      "{'W': array([[6.38632924]]), 'B': array([5.7572262])}\n",
      "after\n",
      "{'W': array([[7.27000274]]), 'B': array([5.87271054])}\n",
      "\n",
      " 6 ~ 8 열\n",
      "[[-4.2304668 ]\n",
      " [ 3.00619632]]\n",
      "[[-22.56309671]\n",
      " [ 25.47510763]]\n",
      "before\n",
      "{'W': array([[7.27000274]]), 'B': array([5.87271054])}\n",
      "after\n",
      "{'W': array([[6.93829597]]), 'B': array([5.87405133])}\n",
      "Loss 1.208490841849526\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "'''\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "'''\n",
    "#print(result[:,0])\n",
    "\n",
    "def linear_regression(minibatch_size, epoch_size):\n",
    "    R=10\n",
    "    size=1\n",
    "    weights={}\n",
    "    #data_set={}    \n",
    "    loss_grad={}    # dJdW, dJdB 저장공간\n",
    "    forward_info={} # 순방향 저장공간\n",
    "    batch={}\n",
    "    \n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(10):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "        \n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "    \n",
    "    weights['W']=W\n",
    "    weights['B']=b\n",
    "\n",
    "    result=np.concatenate((x,y),axis=1)\n",
    "       \n",
    "    train_idx=int(result.shape[0]*0.85)\n",
    "    dev_idx=int(result.shape[0]*0.05)\n",
    "    test_idx=int(result.shape[0]*0.1)\n",
    "    \n",
    "    train_data_set=result[0:train_idx,:]\n",
    "    test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "    dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        result=np.random.permutation(result)\n",
    "        \n",
    "        X_batch= train_data_set[:,0]\n",
    "        y_batch= train_data_set[:,1]\n",
    "        X_batch=np.reshape(X_batch,(train_idx,size))\n",
    "        y_batch=np.reshape(y_batch,(train_idx,size))\n",
    "\n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        print('result')\n",
    "        print(result)\n",
    "        \n",
    "        print(weights)\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            \n",
    "           \n",
    "            print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            print(X_batch1)\n",
    "            print(y1)\n",
    "            \n",
    "            N=weights['W']*X_batch1\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y1-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch1\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y1 # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "            print('before')\n",
    "            print(weights)\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.01 * loss_grad[key]\n",
    "\n",
    "            print('after')\n",
    "            print(weights)\n",
    "    \n",
    "        N=weights['W']*X_batch1\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y1-f,2))\n",
    "        print('Loss',loss)\n",
    "        print('=================================')\n",
    "linear_regression(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* 1 번차 epoch *************\n",
      "Loss 40497306385.75362\n",
      "=================================\n",
      "************* 2 번차 epoch *************\n",
      "Loss 1.9911340988523707e+23\n",
      "=================================\n",
      "************* 3 번차 epoch *************\n",
      "Loss 9.789804333103506e+35\n",
      "=================================\n",
      "************* 4 번차 epoch *************\n",
      "Loss 4.8133507901655004e+48\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "'''\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "'''\n",
    "#print(result[:,0])\n",
    "\n",
    "def linear_regression(minibatch_size, epoch_size):\n",
    "    R=10\n",
    "    size=1\n",
    "    weights={}\n",
    "    #data_set={}    \n",
    "    loss_grad={}    # dJdW, dJdB 저장공간\n",
    "    forward_info={} # 순방향 저장공간\n",
    "    batch={}\n",
    "    \n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(1000):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "        \n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "    \n",
    "    weights['W']=W\n",
    "    weights['B']=b\n",
    "\n",
    "    result=np.concatenate((x,y),axis=1)\n",
    "       \n",
    "    train_idx=int(result.shape[0]*0.85)\n",
    "    dev_idx=int(result.shape[0]*0.05)\n",
    "    test_idx=int(result.shape[0]*0.1)\n",
    "    \n",
    "    train_data_set=result[0:train_idx,:]\n",
    "    test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "    dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "    \n",
    "    '''\n",
    "#    data_set['train_data_set']=train_data_set\n",
    "#    data_set['dev_data_set']=dev_data_set\n",
    "#    data_set['test_data_set']=test_data_set\n",
    "\n",
    "    X_batch= train_data_set[:,0]\n",
    "    y_batch= train_data_set[:,1]\n",
    "    X_batch=np.reshape(X_batch,(train_idx,size))\n",
    "    y_batch=np.reshape(y_batch,(train_idx,size))\n",
    "\n",
    "#    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "#    assert X_batch.shape[1] == weights['W'].shape[0]'''\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*************',j,'번차 epoch *************')\n",
    "        result=np.random.permutation(result)\n",
    "   \n",
    "        X_batch= train_data_set[:,0]\n",
    "        y_batch= train_data_set[:,1]\n",
    "        X_batch=np.reshape(X_batch,(train_idx,size))\n",
    "        y_batch=np.reshape(y_batch,(train_idx,size))\n",
    "\n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            #print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            \n",
    "            \n",
    "            #80~104까지 하나의 함수로 만들면 될거같은데. 그러면 train,test,dev의 mse를 구할 수 있을거 같다.\n",
    "            #함수(데이터,number_minibatch+1)\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            N=weights['W']*X_batch1\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y1-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch1\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y1 # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        \n",
    "        N=weights['W']*X_batch1\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y1-f,2))\n",
    "        print('Loss',loss)\n",
    "        print('=================================')\n",
    "linear_regression(90,4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
