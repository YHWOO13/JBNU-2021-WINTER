{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 MLP - Stochastic Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 3. 19. 2021 - 5. 24. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "  \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset 1번째 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "print(np.max(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self,input_size,output_size):\n",
    "#         self.w = np.random.randn(input_size,output_size)\n",
    "#         self.b = np.random.randn(output_size)\n",
    "        \n",
    "        self.w = np. random.randn(input_size,output_size)\n",
    "        self.b = np.zeros(output_size)\n",
    "        self.dydw = None\n",
    "        self.dydb = 1 \n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.dydw = X.T\n",
    "        affine = np.dot(X,self.w) + self.b\n",
    "        \n",
    "        return affine\n",
    "    \n",
    "    def backward(self,dLdy,lr):\n",
    "        dLdw = np.dot(self.dydw,dLdy)\n",
    "        dLdb = np.sum(np.dot(dLdy, self.dydb),axis=0)\n",
    "        \n",
    "        dLdx = np.dot(dLdy,np.transpose(self.w))\n",
    "       \n",
    "        self.w -= lr * dLdw\n",
    "        \n",
    "        self.b -= lr * dLdb\n",
    "        \n",
    "        return dLdx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Relu class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "#         print('dout',dout.shape)\n",
    "#         print('mask',self.mask.shape)\n",
    "        dout[self.mask]= 0\n",
    "        #dout * np.where(self.mask==True,1,0)\n",
    "        return dout * np.where(self.mask == True,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 마지막\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        #layer 설정\n",
    "        self.layer=[784]\n",
    "        L = int(input('Number of Hidden_Layer: '))\n",
    "        for i in range(L):\n",
    "        #     print(i+1,'th Dimenson', end = ' ') \n",
    "        #     print(end='')\n",
    "            d = int(input('Dimension:'))\n",
    "            self.layer.append(d)    \n",
    "        self.length = len(self.layer)\n",
    "       \n",
    "        for i in range(self.length-1):\n",
    "            self.layers.append(Layer(self.layer[i],self.layer[i+1]))\n",
    "            self.layers.append(Relu())\n",
    "        self.layers.append(Layer(self.layer[-1],10))\n",
    "    \n",
    "    def forward(self,put):\n",
    "        output = self.layers[0].forward(put)\n",
    "        for i in range(1,len(self.layers)):            \n",
    "            output = self.layers[i].forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dld_, lr):\n",
    "        for i in range(self.length-1,0,-1):\n",
    "            dld_ = self.layers[i*2].backward(dld_, lr)\n",
    "            dld_ = self.layers[i*2-1].backward(dld_)\n",
    "        dld_ = self.layers[0].backward(dld_,lr)\n",
    "\n",
    "        return dld_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Layer:  3\n",
      "Dimension: 100\n",
      "Dimension: 40\n",
      "Dimension: 30\n"
     ]
    }
   ],
   "source": [
    "mode = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make softmax class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "class Softmax():    \n",
    "    def __init__(self):\n",
    "        self.exp_o = 0\n",
    "        self.sum_exp_o = 0\n",
    "        self.delta = 0\n",
    "        \n",
    "    def forward(self,h):\n",
    "        self.delta = np.max(h)\n",
    "        \n",
    "        self.exp_o = np.exp(h - self.delta)\n",
    "        self.sum_exp_o = np.sum(self.exp_o)\n",
    "        self.softmax = self.exp_o / self.sum_exp_o\n",
    "        \n",
    "        return self.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Cross_entropy class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "class CE():\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        \n",
    "    def forward(self, softMAX, y):\n",
    "        # cross_entropy + 마이너스 무한대가 생기지 않도록 만듦\n",
    "        delta = 1e-13\n",
    "        \n",
    "        return - np.sum(y * np.log(softMAX + delta))\n",
    "        \n",
    "    def backward(self, y, softMAX):\n",
    "        batch_size = y.shape[0]\n",
    "        dJdR = (softMAX-y) / batch_size\n",
    "        return dJdR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax - with - Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxwithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def softmax(self,a):\n",
    "        c = np.max(a,axis=-1,keepdims=True)\n",
    "        exp_a = np.exp(a-c)\n",
    "        sum_exp_a = np.sum(exp_a,axis=-1,keepdims=True)\n",
    "        y = exp_a / sum_exp_a\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def cross_entropy_error(self,y,t):\n",
    "        delta = 1e-13\n",
    "        return -np.sum(t * np.log(y + delta)) / y.shape[0]\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "print(np.max(a,axis=-1,keepdims=True))\n",
    "\n",
    "sml = SoftmaxwithLoss()\n",
    "b = sml.softmax(a)\n",
    "print(b)\n",
    "print(np.sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    if target.ndim != 1 :\n",
    "        target = np.argmax(target,axis=1)\n",
    "        \n",
    "    accuracy = np.sum(pred == target) / float(pred.shape[0])\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def learning(model,loss_func,X,y, minibatch_size, epoch_size, learning_rate):\n",
    "    acc_list = []\n",
    "    \n",
    "    for e in range(1,epoch_size+1):\n",
    "        \n",
    "        combined = list(zip(X, y))\n",
    "        permut = np.random.permutation(combined)\n",
    "        X[:], y[:] = zip(*permut)\n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X.shape[0] / minibatch_size))\n",
    "        \n",
    "        for n in range(1, number_minibatch+1):\n",
    "\n",
    "            X_temp=X[minibatch_size * (n-1):minibatch_size*n]\n",
    "            y_temp=y[minibatch_size * (n-1):minibatch_size*n]\n",
    "            \n",
    "            c = model.forward(X_temp)\n",
    "            J = loss_func.forward(c,y_temp)\n",
    "        \n",
    "            dJdh = loss_func.backward()\n",
    "            model.backward(dJdh,learning_rate)\n",
    "\n",
    " \n",
    "        c = model.forward(X)\n",
    "        J = loss_func.forward(c,y)\n",
    "\n",
    "\n",
    "        acc = accuracy(c, y)\n",
    "\n",
    "        acc_list.append(acc)\n",
    "        if e % iter_per_epoch == 0:\n",
    "            print('*************',e,'번차 epoch *************')\n",
    "    #             print('최종')\n",
    "    #             print(J)\n",
    "#             print('정확도')\n",
    "#             print(acc)\n",
    "        \n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Layer:  2\n",
      "Dimension: 58\n",
      "Dimension: 33\n",
      "Epeoch_size: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* 10 번차 epoch *************\n",
      "정확도\n",
      "0.45765\n",
      "************* 20 번차 epoch *************\n",
      "정확도\n",
      "0.6535833333333333\n",
      "************* 30 번차 epoch *************\n",
      "정확도\n",
      "0.7666166666666666\n",
      "************* 40 번차 epoch *************\n",
      "정확도\n",
      "0.78555\n",
      "************* 50 번차 epoch *************\n",
      "정확도\n",
      "0.8476833333333333\n",
      "************* 60 번차 epoch *************\n",
      "정확도\n",
      "0.8690333333333333\n",
      "************* 70 번차 epoch *************\n",
      "정확도\n",
      "0.8928166666666667\n",
      "************* 80 번차 epoch *************\n",
      "정확도\n",
      "0.9294333333333333\n",
      "************* 90 번차 epoch *************\n",
      "정확도\n",
      "0.9426333333333333\n",
      "************* 100 번차 epoch *************\n",
      "정확도\n",
      "0.95525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13635,\n",
       " 0.1737,\n",
       " 0.23818333333333333,\n",
       " 0.30078333333333335,\n",
       " 0.31071666666666664,\n",
       " 0.37223333333333336,\n",
       " 0.35615,\n",
       " 0.40166666666666667,\n",
       " 0.41846666666666665,\n",
       " 0.45765,\n",
       " 0.46885,\n",
       " 0.50585,\n",
       " 0.5250333333333334,\n",
       " 0.5445666666666666,\n",
       " 0.5878,\n",
       " 0.6001666666666666,\n",
       " 0.6133333333333333,\n",
       " 0.6279166666666667,\n",
       " 0.63955,\n",
       " 0.6535833333333333,\n",
       " 0.6972833333333334,\n",
       " 0.7186833333333333,\n",
       " 0.7161166666666666,\n",
       " 0.7246,\n",
       " 0.72565,\n",
       " 0.7426333333333334,\n",
       " 0.7395666666666667,\n",
       " 0.7591666666666667,\n",
       " 0.7879333333333334,\n",
       " 0.7666166666666666,\n",
       " 0.7595,\n",
       " 0.7601,\n",
       " 0.7697166666666667,\n",
       " 0.7624666666666666,\n",
       " 0.77075,\n",
       " 0.7749333333333334,\n",
       " 0.76855,\n",
       " 0.7770666666666667,\n",
       " 0.7956,\n",
       " 0.78555,\n",
       " 0.7876,\n",
       " 0.81645,\n",
       " 0.8212333333333334,\n",
       " 0.8327666666666667,\n",
       " 0.8378333333333333,\n",
       " 0.8248333333333333,\n",
       " 0.8330833333333333,\n",
       " 0.8364166666666667,\n",
       " 0.8448333333333333,\n",
       " 0.8476833333333333,\n",
       " 0.8451333333333333,\n",
       " 0.8467166666666667,\n",
       " 0.8417166666666667,\n",
       " 0.8398166666666667,\n",
       " 0.8594333333333334,\n",
       " 0.8560333333333333,\n",
       " 0.855,\n",
       " 0.8623166666666666,\n",
       " 0.8583166666666666,\n",
       " 0.8690333333333333,\n",
       " 0.8731166666666667,\n",
       " 0.8745166666666667,\n",
       " 0.8801,\n",
       " 0.8733166666666666,\n",
       " 0.8815333333333333,\n",
       " 0.8780166666666667,\n",
       " 0.8829166666666667,\n",
       " 0.8892166666666667,\n",
       " 0.8899,\n",
       " 0.8928166666666667,\n",
       " 0.8947333333333334,\n",
       " 0.90235,\n",
       " 0.90555,\n",
       " 0.90495,\n",
       " 0.90115,\n",
       " 0.9259333333333334,\n",
       " 0.9257,\n",
       " 0.9330166666666667,\n",
       " 0.9251333333333334,\n",
       " 0.9294333333333333,\n",
       " 0.9316666666666666,\n",
       " 0.9332333333333334,\n",
       " 0.9298666666666666,\n",
       " 0.9295833333333333,\n",
       " 0.9318666666666666,\n",
       " 0.93185,\n",
       " 0.9418333333333333,\n",
       " 0.9427166666666666,\n",
       " 0.94325,\n",
       " 0.9426333333333333,\n",
       " 0.9544666666666667,\n",
       " 0.95725,\n",
       " 0.9579833333333333,\n",
       " 0.9558833333333333,\n",
       " 0.95725,\n",
       " 0.9569333333333333,\n",
       " 0.95755,\n",
       " 0.9561666666666667,\n",
       " 0.9539833333333333,\n",
       " 0.95525]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "swl = SoftmaxwithLoss()\n",
    "\n",
    "minibatch_size = 100\n",
    "iter_per_epoch = 10\n",
    "\n",
    "\n",
    "epoch_size = int(input('Epeoch_size:'))\n",
    "train_result = learning(model, swl, x_train, t_train, minibatch_size, epoch_size, 0.0001)\n",
    "test_result = learning(model, swl, x_test, t_test, minibatch_size, epoch_size, 0.0001)\n",
    "learning(model, swl, x_train, t_train, minibatch_size, epoch_size, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Layer:  1\n",
      "Dimension: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* 10 번차 epoch *************\n",
      "정확도\n",
      "0.9968\n",
      "************* 20 번차 epoch *************\n",
      "정확도\n",
      "0.9996833333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-cc0469f48a74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mswl2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-de2411264e18>\u001b[0m in \u001b[0;36mlearning\u001b[1;34m(model, loss_func, X, y, minibatch_size, epoch_size, learning_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mdJdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdJdh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-fa22e20b66c5>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dld_, lr)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mdld_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdld_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mdld_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdld_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdld_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdld_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdld_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d7e90a7d5ba1>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dLdy, lr)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mdLdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdLdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdydb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdLdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdLdy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdLdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2 = Model()\n",
    "swl2 = SoftmaxwithLoss()\n",
    "\n",
    "minibatch_size = 100\n",
    "epoch_size = 130\n",
    "iter_per_epoch = 10\n",
    "\n",
    "learning(model2,swl2,x_train, t_train, minibatch_size, epoch_size, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
