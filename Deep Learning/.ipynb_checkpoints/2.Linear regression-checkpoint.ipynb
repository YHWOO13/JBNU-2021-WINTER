{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 1. 8. 2020 - 2. 5. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Stochastic Gradient Descent (SGD) Method with Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "=============\n",
      "\n",
      " 0 ~ 3 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "f \n",
      " [[ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n",
      "Loss: 230\n",
      "{'W': array([[160],\n",
      "       [120],\n",
      "       [180]]), 'B': array([60])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.94]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 4 ~ 7 열\n",
      "N \n",
      " [[ 8.42]\n",
      " [ 9.26]\n",
      " [10.1 ]\n",
      " [10.94]]\n",
      "f \n",
      " [[ 9.36]\n",
      " [10.2 ]\n",
      " [11.04]\n",
      " [11.88]]\n",
      "Loss: 373.70559999999995\n",
      "{'W': array([[508.64],\n",
      "       [153.92],\n",
      "       [230.88]]), 'B': array([76.96])}\n",
      "W\n",
      "before\n",
      "[[0.84]\n",
      " [0.88]\n",
      " [0.82]]\n",
      "after\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94]]\n",
      "after\n",
      "[[0.86304]]\n",
      "\n",
      "\n",
      "=================================\n",
      "\n",
      " 8 ~ 11 열\n",
      "N \n",
      " [[6.20176]]\n",
      "f \n",
      " [[7.0648]]\n",
      "Loss: 36.78179904\n",
      "{'W': array([[109.1664],\n",
      "       [ 24.2592],\n",
      "       [ 36.3888]]), 'B': array([12.1296])}\n",
      "W\n",
      "before\n",
      "[[0.33136]\n",
      " [0.72608]\n",
      " [0.58912]]\n",
      "after\n",
      "[[0.2221936]\n",
      " [0.7018208]\n",
      " [0.5527312]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.86304]]\n",
      "after\n",
      "[[0.8509104]]\n",
      "\n",
      "\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN # 뒷계산을 위한 중간 계산\n",
    "    '''\n",
    "       3X1 과 3X1을 어떻게 곱해지는지 이해가 안됐지만 그 이유가 '*'와 'np.dot'의 차이를 알아야한다.\n",
    "       어짜피 연쇄법칙으로 쭉 곱해가면서 가야하는데 위의 식을 곱하기 위해선 무조건 행렬이라고\n",
    "       dot의 개념을 곱하면 안되고 똑같이 곱하되 numpy.array의 성질을 이용(같은 열과 같은 행이면\n",
    "       위와 같이 곱해됨)한다.\n",
    "       그래서 위에서는 '*'을 이용했지만 밑에서는 np.dot을 이용한다(즉, 곱해야 하는 행렬의 꼴을 봐가면서 \n",
    "       '*' 혹은 'np.dot'을 이용하면 된다고 판단. 허나 만약 이도저도 안된다면....이 아니라 만약 그렇다면\n",
    "       아마 이 계산과정 자체가 나오지 않았을 것) '''\n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "print('=============')\n",
    "\n",
    "minibatch_size = 4\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "\n",
    "for i in range(1, number_minibatch+1):\n",
    "    print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "    matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "    loss_gradient(matrix1,y1,weights)\n",
    "    \n",
    "    for key in weights.keys():\n",
    "        print(key)\n",
    "        print('before')\n",
    "        print(weights[key])\n",
    "        weights[key]=weights[key]- 0.001 * loss_grad[key]\n",
    "        print('after')\n",
    "        print(weights[key])\n",
    "        print('\\n')\n",
    "    print('=================================')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 early stopping 추가한 Algorithm of mini-batch SGD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭의 수를 정해 다 돌기 전에 끝내는 것 => 끝내는 이유는 모델이 너무 training data에 치우쳐 져 있어서 test data나\n",
    "real data에서 모델의 성능이 낮아지는 경향을 막기 위해서 <br>\n",
    "\n",
    "1. 데이터를 training data, validation data, test data로 나눈다 <br>\n",
    "2. 학습데이터만을 가지고 epoch이 한번 끝날 때마다 validation data로 시험을 해보는 것 <br>\n",
    "3. validation accuracy가 증가하다가 계속 낮아지면 이때 학습을 멈추는 것(즉, validation accuracy가 최대일 때 멈춘다) <br>\n",
    "4. 이후 이 모델(validation accuray가 젤 높은 모델)에 test data를 이용하여 확인 해 보는 것 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, <br>\n",
    "2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, <br>\n",
    "3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 <br> \n",
    "   => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.<br>\n",
    "\n",
    "※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다. <br>\n",
    "<br>\n",
    "얼마나 신기하냐면<br>\n",
    "random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.<br>\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Data_set\n",
      "[[-4.57053845 38.33651852]\n",
      " [-8.54666509 65.86879823]\n",
      " [-0.90103712 12.48693641]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(N,size):\n",
    "    #size==특징의 개수\n",
    "    R = 10\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result = np.concatenate((x,y),axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result=data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print('test_Data_set')\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 Linear regression(Random data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "batch gradient descent\n",
      "******* 1 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[6]\n",
      " [7]\n",
      " [8]]\n",
      "f \n",
      " [[7]\n",
      " [8]\n",
      " [9]]\n",
      "Loss: 149\n",
      "{'W': array([[ 88],\n",
      "       [ 84],\n",
      "       [126]]), 'B': array([42])}\n",
      "W\n",
      "before\n",
      "[[1]\n",
      " [1]\n",
      " [1]]\n",
      "after\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[1]]\n",
      "after\n",
      "[[0.9958]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.9102]\n",
      " [ 9.9014]\n",
      " [10.8926]]\n",
      "f \n",
      " [[ 9.906 ]\n",
      " [10.8972]\n",
      " [11.8884]]\n",
      "Loss: 295.8286584\n",
      "{'W': array([[300.8808],\n",
      "       [118.7664],\n",
      "       [178.1496]]), 'B': array([59.3832])}\n",
      "W\n",
      "before\n",
      "[[0.9912]\n",
      " [0.9916]\n",
      " [0.9874]]\n",
      "after\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9958]]\n",
      "after\n",
      "[[0.98986168]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[11.59598528]\n",
      " [12.5570972 ]\n",
      " [13.51820912]]\n",
      "f \n",
      " [[12.58584696]\n",
      " [13.54695888]\n",
      " [14.5080708 ]]\n",
      "Loss: 474.1260036547649\n",
      "{'W': array([[606.09847392],\n",
      "       [150.56350656],\n",
      "       [225.84525984]]), 'B': array([75.28175328])}\n",
      "W\n",
      "before\n",
      "[[0.96111192]\n",
      " [0.97972336]\n",
      " [0.96958504]]\n",
      "after\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.98986168]]\n",
      "after\n",
      "[[0.9823335]]\n",
      "\n",
      "\n",
      "******* 2 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.67083763]\n",
      " [6.57133971]\n",
      " [7.47184178]]\n",
      "f \n",
      " [[6.65317114]\n",
      " [7.55367321]\n",
      " [8.45417528]]\n",
      "Loss: 130.47370562049468\n",
      "{'W': array([[ 82.24608682],\n",
      "       [ 78.64407853],\n",
      "       [117.96611779]]), 'B': array([39.32203926])}\n",
      "W\n",
      "before\n",
      "[[0.90050207]\n",
      " [0.96466701]\n",
      " [0.94700051]]\n",
      "after\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9823335]]\n",
      "after\n",
      "[[0.9784013]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[ 8.28832677]\n",
      " [ 9.18060423]\n",
      " [10.07288169]]\n",
      "f \n",
      " [[ 9.26672807]\n",
      " [10.15900553]\n",
      " [11.05128299]]\n",
      "Loss: 253.25446504480863\n",
      "{'W': array([[278.33927576],\n",
      "       [109.90806636],\n",
      "       [164.86209954]]), 'B': array([54.95403318])}\n",
      "W\n",
      "before\n",
      "[[0.89227746]\n",
      " [0.9568026 ]\n",
      " [0.9352039 ]]\n",
      "after\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9784013]]\n",
      "after\n",
      "[[0.9729059]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[10.69888142]\n",
      " [11.56332496]\n",
      " [12.42776849]]\n",
      "f \n",
      " [[11.67178732]\n",
      " [12.53623085]\n",
      " [13.40067439]]\n",
      "Loss: 400.74839226007464\n",
      "{'W': array([[557.19685518],\n",
      "       [138.43477026],\n",
      "       [207.65215539]]), 'B': array([69.21738513])}\n",
      "W\n",
      "before\n",
      "[[0.86444354]\n",
      " [0.94581179]\n",
      " [0.91871769]]\n",
      "after\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.9729059]]\n",
      "after\n",
      "[[0.96598416]]\n",
      "\n",
      "\n",
      "******* 3 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.36651792]\n",
      " [6.17524177]\n",
      " [6.98396562]]\n",
      "f \n",
      " [[6.33250208]\n",
      " [7.14122593]\n",
      " [7.94994978]]\n",
      "Loss: 114.45203617371786\n",
      "{'W': array([[ 76.92960652],\n",
      "       [ 73.69471112],\n",
      "       [110.54206668]]), 'B': array([36.84735556])}\n",
      "W\n",
      "before\n",
      "[[0.80872385]\n",
      " [0.93196832]\n",
      " [0.89795248]]\n",
      "after\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96598416]]\n",
      "after\n",
      "[[0.96229942]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.71401606]\n",
      " [8.51504695]\n",
      " [9.31607784]]\n",
      "f \n",
      " [[ 8.67631549]\n",
      " [ 9.47734638]\n",
      " [10.27837727]]\n",
      "Loss: 216.87950580813276\n",
      "{'W': array([[257.5245149 ],\n",
      "       [101.72815654],\n",
      "       [152.5922348 ]]), 'B': array([50.86407827])}\n",
      "W\n",
      "before\n",
      "[[0.80103089]\n",
      " [0.92459885]\n",
      " [0.88689827]]\n",
      "after\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.96229942]]\n",
      "after\n",
      "[[0.95721302]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.87071827]\n",
      " [10.64599671]\n",
      " [11.42127515]]\n",
      "f \n",
      " [[10.82793129]\n",
      " [11.60320973]\n",
      " [12.37848817]]\n",
      "Loss: 338.4862828577476\n",
      "{'W': array([[512.05518065],\n",
      "       [127.23851672],\n",
      "       [190.85777509]]), 'B': array([63.61925836])}\n",
      "W\n",
      "before\n",
      "[[0.77527844]\n",
      " [0.91442603]\n",
      " [0.87163905]]\n",
      "after\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95721302]]\n",
      "after\n",
      "[[0.95085109]]\n",
      "\n",
      "\n",
      "******* 4 번차 epoch*******\n",
      "\n",
      " 0 ~ 2 열\n",
      "N \n",
      " [[5.08513709]\n",
      " [5.80921001]\n",
      " [6.53328293]]\n",
      "f \n",
      " [[6.03598818]\n",
      " [6.7600611 ]\n",
      " [7.48413402]]\n",
      "Loss: 100.58347471209066\n",
      "{'W': array([[ 72.01702485],\n",
      "       [ 69.12073316],\n",
      "       [103.68109975]]), 'B': array([34.56036658])}\n",
      "W\n",
      "before\n",
      "[[0.72407292]\n",
      " [0.90170218]\n",
      " [0.85255327]]\n",
      "after\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.95085109]]\n",
      "after\n",
      "[[0.94739505]]\n",
      "\n",
      "\n",
      "\n",
      " 3 ~ 5 열\n",
      "N \n",
      " [[7.18362056]\n",
      " [7.90049178]\n",
      " [8.617363  ]]\n",
      "f \n",
      " [[8.13101562]\n",
      " [8.84788683]\n",
      " [9.56475805]]\n",
      "Loss: 185.7957919285563\n",
      "{'W': array([[238.30408987],\n",
      "       [ 94.174642  ],\n",
      "       [141.261963  ]]), 'B': array([47.087321])}\n",
      "W\n",
      "before\n",
      "[[0.71687122]\n",
      " [0.89479011]\n",
      " [0.84218516]]\n",
      "after\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94739505]]\n",
      "after\n",
      "[[0.94268632]]\n",
      "\n",
      "\n",
      "\n",
      " 6 ~ 8 열\n",
      "N \n",
      " [[ 9.10620784]\n",
      " [ 9.79924865]\n",
      " [10.49228945]]\n",
      "f \n",
      " [[10.04889416]\n",
      " [10.74193497]\n",
      " [11.43497578]]\n",
      "Loss: 285.6765017928727\n",
      "{'W': array([[470.38504162],\n",
      "       [116.9032196 ],\n",
      "       [175.35482939]]), 'B': array([58.4516098])}\n",
      "W\n",
      "before\n",
      "[[0.69304081]\n",
      " [0.88537264]\n",
      " [0.82805896]]\n",
      "after\n",
      "[[0.6460023 ]\n",
      " [0.87368232]\n",
      " [0.81052348]]\n",
      "\n",
      "\n",
      "B\n",
      "before\n",
      "[[0.94268632]]\n",
      "after\n",
      "[[0.93684116]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.4.3 초기버전\n",
    "matrix= [[1,2,3],[2,2,3],[3,2,3],[4,2,3],[5,2,3],[6,2,3],[7,2,3],[8,2,3],[9,2,3]]\n",
    "matrix=np.array(matrix)\n",
    "y = [[1],[1],[1],[1],[1],[1],[1],[1],[1]]\n",
    "y=np.array(y)\n",
    "w = [[1],[1],[1]]\n",
    "w=np.array(w)\n",
    "B = np.random.randint(1,2,(1,1))\n",
    "weights = {'W':w,'B':B} \n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, weights):\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]==1\n",
    "    \n",
    "    N=np.dot(X_batch,weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    print('N \\n',N)\n",
    "    print('f \\n',f)\n",
    "    print('Loss:',loss)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    \n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "            \n",
    "    return print(loss_grad)\n",
    "\n",
    "#loss_gradient(matrix,y,weights)\n",
    "\n",
    "print('==================================================================')\n",
    "\n",
    "print('batch gradient descent')\n",
    "def batch(loss_grad):\n",
    "    for i in range(1,4):\n",
    "        print('\\n')\n",
    "        print(i,'회 반복')\n",
    "        for key in weights.keys():\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key] = weights[key]- 0.001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "#batch(loss_grad)\n",
    "\n",
    "minibatch_size = 3\n",
    "number_minibatch= np.int(np.ceil(matrix.shape[0]/minibatch_size))\n",
    "epoch_size=4\n",
    "\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        matrix1=matrix[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        loss_gradient(matrix1,y1,weights)\n",
    "\n",
    "        for key in weights.keys():\n",
    "            print(key)\n",
    "            print('before')\n",
    "            print(weights[key])\n",
    "            weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "            print('after')\n",
    "            print(weights[key])\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.3 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.980506880774058\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.9803617309142036\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.9803345206129653\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.9803187114550124\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.9803144335541265\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.9803044687548884\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.9802985855866283\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.9802851998448713\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.9802788019743855\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.980272589704833\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.9802673357722426\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.9802650164618018\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.9802576240666173\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.9802526399908201\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.9802478910826566\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.9802596543593757\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.9802492423649862\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.9802392846981962\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.9802366969926353\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.9802456116133814\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.9802238578794905\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.9802186469996219\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.9802281547788441\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.9802200140569051\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.9802105004551307\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.9802061313548227\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.9802184551213058\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.9801991334469122\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.9802042925278809\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.9802025913912017\n",
      "\n",
      "Test_data\n",
      "************* 1 번차 epoch *************\n",
      "Loss 0.9593238864842596\n",
      "************* 2 번차 epoch *************\n",
      "Loss 0.955306583127813\n",
      "************* 3 번차 epoch *************\n",
      "Loss 0.9518139060660252\n",
      "************* 4 번차 epoch *************\n",
      "Loss 0.9488795958377731\n",
      "************* 5 번차 epoch *************\n",
      "Loss 0.9462630023316911\n",
      "************* 6 번차 epoch *************\n",
      "Loss 0.944024182119898\n",
      "************* 7 번차 epoch *************\n",
      "Loss 0.9422072797559247\n",
      "************* 8 번차 epoch *************\n",
      "Loss 0.9406410102881588\n",
      "************* 9 번차 epoch *************\n",
      "Loss 0.9393044207955232\n",
      "************* 10 번차 epoch *************\n",
      "Loss 0.9380812727558147\n",
      "************* 11 번차 epoch *************\n",
      "Loss 0.9370633659125259\n",
      "************* 12 번차 epoch *************\n",
      "Loss 0.9362440649841121\n",
      "************* 13 번차 epoch *************\n",
      "Loss 0.9354991818999596\n",
      "************* 14 번차 epoch *************\n",
      "Loss 0.9348800702587561\n",
      "************* 15 번차 epoch *************\n",
      "Loss 0.9343486070994748\n",
      "************* 16 번차 epoch *************\n",
      "Loss 0.9338976121223194\n",
      "************* 17 번차 epoch *************\n",
      "Loss 0.9334808668150081\n",
      "************* 18 번차 epoch *************\n",
      "Loss 0.9331269770338102\n",
      "************* 19 번차 epoch *************\n",
      "Loss 0.9328191837527748\n",
      "************* 20 번차 epoch *************\n",
      "Loss 0.9325500719895523\n",
      "************* 21 번차 epoch *************\n",
      "Loss 0.932334350571692\n",
      "************* 22 번차 epoch *************\n",
      "Loss 0.9321389302754777\n",
      "************* 23 번차 epoch *************\n",
      "Loss 0.9319758664352236\n",
      "************* 24 번차 epoch *************\n",
      "Loss 0.9318427277215957\n",
      "************* 25 번차 epoch *************\n",
      "Loss 0.9317249066422645\n",
      "************* 26 번차 epoch *************\n",
      "Loss 0.9316129731546274\n",
      "************* 27 번차 epoch *************\n",
      "Loss 0.9315215500556809\n",
      "************* 28 번차 epoch *************\n",
      "Loss 0.9314400083614233\n",
      "************* 29 번차 epoch *************\n",
      "Loss 0.9313695130073519\n",
      "************* 30 번차 epoch *************\n",
      "Loss 0.9313098235997843\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "xlis = []\n",
    "ylis = []\n",
    "\n",
    "R=10\n",
    "size=1\n",
    "weights={}\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "batch={}\n",
    "\n",
    "W= np.random.uniform(-R,R,size=size)\n",
    "b= np.random.uniform(-R,R,size=size)\n",
    "b= random.choice(b)\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.random.uniform(-R,R,size=size)\n",
    "    y = np.random.normal(W*x+b,1,size=size)\n",
    "    xlis.append(x)\n",
    "    ylis.append(y)\n",
    "    flis.append(W*x+b)\n",
    "\n",
    "x=np.array(xlis)\n",
    "y=np.array(ylis)\n",
    "\n",
    "weights['W']=W\n",
    "weights['B']=b\n",
    "\n",
    "result=np.concatenate((x,y),axis=1)\n",
    "\n",
    "train_idx=int(result.shape[0]*0.85)\n",
    "dev_idx=int(result.shape[0]*0.05)\n",
    "test_idx=int(result.shape[0]*0.1)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "def linear_regression(data, idx, minibatch_size, epoch_size):\n",
    "    data_list=[]\n",
    "    X_batch= data[:,0]\n",
    "    y_batch= data[:,1]\n",
    "    X_batch=np.reshape(X_batch,(idx,size))\n",
    "    y_batch=np.reshape(y_batch,(idx,size))\n",
    "\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*************',j,'번차 epoch *************')\n",
    "        data=np.random.permutation(data)\n",
    "        X_batch= data[:,0]\n",
    "        y_batch= data[:,1]\n",
    "        X_batch=np.reshape(X_batch,(idx,size))\n",
    "        y_batch=np.reshape(y_batch,(idx,size))\n",
    "        \n",
    "        number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    \n",
    "    #    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    #    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "        \n",
    "        for i in range(1, number_minibatch+1):\n",
    "            X_batch_temp=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y_temp=y_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            \n",
    "            N=weights['W']*X_batch_temp\n",
    "            f= N+weights['B']\n",
    "            loss=np.mean(np.power(y_temp-f,2))\n",
    "            \n",
    "            forward_info['X']= X_batch_temp\n",
    "            forward_info['N']= N       # \n",
    "            forward_info['f']= f       # 예측값\n",
    "            forward_info['y']= y_temp # 실제값\n",
    "\n",
    "            # 전체코드로 본 도함수 계산과정\n",
    "            batch_size=forward_info['X'].shape[0]\n",
    "            dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "            dfdN=np.ones_like(forward_info['N']) \n",
    "            dfdB=np.ones_like(forward_info['N'])\n",
    "            dJdN=dJdf*dfdN \n",
    "            dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "            dJdW=np.dot(dNdW, dJdN)\n",
    "            dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "            loss_grad['W']=dJdW\n",
    "            loss_grad['B']=dLdB\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.00001 * loss_grad[key]\n",
    "        \n",
    "        N=weights['W']*X_batch\n",
    "        f= N+weights['B']\n",
    "        loss=np.mean(np.power(y_batch-f,2))\n",
    "        print('Loss',loss)\n",
    "      \n",
    "        #print('=================================')\n",
    "        \n",
    "        data_list.append(loss)\n",
    "        \n",
    "    X_batch = list(X_batch)\n",
    "    \n",
    "    #epoch에 따른 loss 출력도 가능\n",
    "    return plt.scatter(X_batch_temp, y_temp, label = 'name')\n",
    "\n",
    "print('Train_data')\n",
    "train_ = linear_regression(train_data_set,train_idx,50,30)\n",
    "\n",
    "#print('\\ndev_data')\n",
    "#dev_ = linear_regression(dev_data_set,dev_idx,20,30)\n",
    "\n",
    "print('\\nTest_data')\n",
    "Test_ = linear_regression(test_data_set,test_idx,50,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
