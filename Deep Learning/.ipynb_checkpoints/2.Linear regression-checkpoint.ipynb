{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " Author: Yoonhyuck WOO / JBNU_Industrial Information system Engineering\n",
    " Date; 12. 29. 2020 - 1. 8. 2021\n",
    " Title: Artificial Intelligence_Project 2\n",
    " Professor: Seung-Hoon Na'''\n",
    "            \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of np.dot\n",
      "[[ 7 10]\n",
      " [15 22]]\n",
      "\n",
      " example of numpy.einsum\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "\n",
      " [[ 7 10]\n",
      " [15 22]]\n"
     ]
    }
   ],
   "source": [
    "# 1.1 - numpy.dot\n",
    "a=[[1,2],[3,4]]\n",
    "b=[[1,2],[3,4]]\n",
    "\n",
    "a=np.array(a,dtype='int')\n",
    "b=np.array(b,dtype='int')\n",
    "\n",
    "print('example of np.dot')\n",
    "c=np.dot(a,b)\n",
    "print(c)\n",
    "\n",
    "# 1.1 - numpy.einsum\n",
    "# script부분에서 행,열을 이용하여 transpose, 대각선, 행렬간의 곱세등 선형대수학적인 연산을 가능하게 해줌.\n",
    "''' ex) np.einsum('ij,jh->ih', a, b) directly specifies the order of the output subscript labelsr\n",
    "      =>returns matrix multiplication'''\n",
    "print('\\n example of numpy.einsum')\n",
    "c=[[1,2,3],[4,5,6]]\n",
    "c=np.array(c)\n",
    "c=np.einsum('ij->ji',c)\n",
    "print(c)\n",
    "#dot과 관련\n",
    "d=np.einsum('ij,jh->ih',a,b)\n",
    "print('\\n',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n67. Considering a four dimensions array, how to get sum over the last two axis at once? (★★★)\\n76. Consider a one-dimensional array Z, \\n    build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) \\n    and each subsequent row is shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) (★★★)\\n77. How to negate a boolean, or to change the sign of a float inplace? (★★★)\\n85. Create a 2D array subclass such that Z[i,j] == Z[j,i] (★★★)\\n94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3]) (★★★)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2 Numpt: Quiz\n",
    "'''\n",
    "67. Considering a four dimensions array, how to get sum over the last two axis at once? (★★★)\n",
    "76. Consider a one-dimensional array Z, \n",
    "    build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) \n",
    "    and each subsequent row is shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1]) (★★★)\n",
    "77. How to negate a boolean, or to change the sign of a float inplace? (★★★)\n",
    "85. Create a 2D array subclass such that Z[i,j] == Z[j,i] (★★★)\n",
    "94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3]) (★★★)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]\n",
      "\n",
      "  [[18 19 20]\n",
      "   [21 22 23]\n",
      "   [24 25 26]]]\n",
      "\n",
      "\n",
      " [[[27 28 29]\n",
      "   [30 31 32]\n",
      "   [33 34 35]]\n",
      "\n",
      "  [[36 37 38]\n",
      "   [39 40 41]\n",
      "   [42 43 44]]\n",
      "\n",
      "  [[45 46 47]\n",
      "   [48 49 50]\n",
      "   [51 52 53]]]]\n",
      "\n",
      "SUM\n",
      "[[ 36 117 198]\n",
      " [279 360 441]]\n"
     ]
    }
   ],
   "source": [
    "# 67. Considering a four dimensions array, how to get sum over the last two axis at once?\n",
    "P_67=np.arange(54).reshape(2,3,3,3)\n",
    "print(P_67)\n",
    "\n",
    "print('\\nSUM')\n",
    "print(P_67.sum(axis=(-2,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 10,  9],\n",
       "       [ 8,  7,  6],\n",
       "       [ 5,  4,  3],\n",
       "       [ 2,  1,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''76 Consider a one-dimensional array Z, \n",
    "    build a two-dimensional array whose first row is (Z[0],Z[1],Z[2]) \n",
    "    and each subsequent row is shifted by 1 (last row should be (Z[-3],Z[-2],Z[-1])\n",
    "'''\n",
    "P_76=np.arange(12).reshape(4,3)\n",
    "P_76[::-1,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "[[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
      "[[False  True  True]\n",
      " [ True False  True]\n",
      " [ True  True False]]\n",
      "\n",
      "before\n",
      "1 : False\n",
      "2 : True\n",
      "3 : True\n",
      "4 : True\n",
      "5 : False\n",
      "6 : True\n",
      "7 : True\n",
      "8 : True\n",
      "9 : False\n",
      "\n",
      "after\n",
      "1 : True\n",
      "2 : False\n",
      "3 : False\n",
      "4 : False\n",
      "5 : True\n",
      "6 : False\n",
      "7 : False\n",
      "8 : False\n",
      "9 : True\n"
     ]
    }
   ],
   "source": [
    "# 77. How to negate a boolean, or to change the sign of a float inplace?\n",
    "# hint: np.logical_not, np.negative\n",
    "# negate= FALSE-> True, True -> False\n",
    "print('before')\n",
    "diago=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "\n",
    "dia=np.logical_not(diago)\n",
    "diag=np.negative(diago)\n",
    "diag1=np.logical_not(diago)\n",
    "print(diago)\n",
    "print(diag1)\n",
    "\n",
    "print('\\nbefore')\n",
    "k1=1\n",
    "for i in diag1:\n",
    "    for j in i:\n",
    "        print(k1,':',j)\n",
    "        k1=k1+1\n",
    "\n",
    "\n",
    "def negate_boolean(x):\n",
    "    k2=1\n",
    "    y=np.logical_not(x)\n",
    "    for i in y:\n",
    "        for j in i:\n",
    "            if j==False:\n",
    "                j=True\n",
    "            elif j==True: \n",
    "                j=False\n",
    "            print(k2,':',j)\n",
    "            k2=k2+1\n",
    "print('\\nafter')\n",
    "negate_boolean(diago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "after\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 85. Create a 2D array subclass such that Z[i,j] == Z[j,i] => diagonal array\n",
    "#diagonal=np.arange(9).reshape(3,3)\n",
    "P_85=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "P_85=np.array(P_85,dtype='int')\n",
    "print('before')\n",
    "print(P_85)\n",
    "\n",
    "print('after')\n",
    "P_85=np.einsum('ij->ji',P_85)\n",
    "print(P_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 6]\n",
      "[6 7 0]\n",
      "[3 6 7]\n",
      "[8 3 5]\n",
      "[0 0 2]\n",
      "[8 5 7]\n",
      "[4 0 5]\n",
      "[3 0 5]\n",
      "[5 3 0]\n",
      "uequal_count 9\n"
     ]
    }
   ],
   "source": [
    "# 94. Considering a 10x3 matrix, extract rows with unequal values (e.g. [2,2,3])\n",
    "P_94=np.random.random((10,3))*10\n",
    "P_94=np.array(P_94,dtype='int')\n",
    "P_94\n",
    "i=0\n",
    "for j in P_94:\n",
    "    if j[0]==j[1]==j[2]:\n",
    "        continue\n",
    "    else:\n",
    "        i=i+1\n",
    "        print(j)\n",
    "print('uequal_count',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[-3 -3]\n",
      " [ 0  1]]\n",
      "\n",
      "B\n",
      "[[6]\n",
      " [5]]\n",
      "\n",
      " result\n",
      "역\n",
      "[[-0.33 -1.  ]\n",
      " [ 0.    1.  ]]\n",
      "A*역 => 역행렬 잘 구했는지 확인\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "해\n",
      "[[-7.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Nupmy: 선형방정식 풀기\n",
    "# np.dot(a, a_inv) => 단위행렬(unit matrix) \n",
    "#pseudo inverse A+는 다음과 같이 계산된다.=>(A+=A^TA)^-1A^T\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "a=random.randrange(2,4)\n",
    "b=random.randrange(2,4)\n",
    "P_13=np.random.randint(-3,4,(a,b))\n",
    "P_13\n",
    "B=np.random.randint(0,10,(a,1))\n",
    "print('A')\n",
    "print(P_13)\n",
    "print('\\nB')\n",
    "print(B)\n",
    "\n",
    "def linearsol(x,y):\n",
    "    x_inv=np.linalg.inv(x)\n",
    "    print('역')\n",
    "    print(x_inv)\n",
    "    \n",
    "    #assert \n",
    "    \n",
    "    unit=np.dot(x_inv,x)\n",
    "    unit=np.array(unit, dtype='int')\n",
    "    print('A*역 => 역행렬 잘 구했는지 확인')\n",
    "    print(unit)\n",
    "    print('\\n해')\n",
    "    return print(np.dot(x_inv,y))\n",
    "\n",
    "def leastsquaresol(a,b,x,y):\n",
    "    if a>b:\n",
    "        x_tran=np.transpose(x)\n",
    "        pseudo_inverse=np.dot(np.linalg.inv(np.dot(x_tran,x)),x_tran)\n",
    "        return print(np.dot(pseudo_inverse,y))\n",
    "    elif a<b:\n",
    "        print('Infinite solutions')\n",
    "    \n",
    "    \n",
    "if a==b:\n",
    "    print('\\n result')\n",
    "    linearsol(P_13,B)\n",
    "else:\n",
    "    print('\\n result')\n",
    "    leastsquaresol(a,b,P_13,B)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 -2 -7]\n",
      " [-4  4 -4]]\n",
      "\n",
      " [[ 5 -4]\n",
      " [-2  4]\n",
      " [-7 -4]]\n",
      "\n",
      " [[ 0  2]\n",
      " [-4 -2]]\n",
      "\n",
      " [[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Linear regression을 위한 식 유도 (행렬에 대한 미분공식 이용)\n",
    "np.random.seed(0)\n",
    "w_row=random.randrange(2,4)\n",
    "w_col=random.randrange(2,4)\n",
    "x_col=random.randrange(2,4)\n",
    "\n",
    "w=np.random.randint(-7,7,(w_row,w_col))\n",
    "w_T=np.transpose(w)\n",
    "x=np.random.randint(-7,7,(w_row,x_col))\n",
    "bias=np.random.randint(0,1,(x_col,1))\n",
    "bias=np.ones_like(bias)\n",
    "\n",
    "print(w)\n",
    "print('\\n',w_T)\n",
    "print('\\n',x)\n",
    "print('\\n',bias)\n",
    "\n",
    "# 2.1-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, \\n    2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, \\n    3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 \\n    => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.\\n\\n    ※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다.\\n    \\n    얼마나 신기하냐면\\n    random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \\n    파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.\\n\\n출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' 1. 시간 혹은 비용 문제로 전수 조사를 못한 상황에서 표본 조사를 해야 할 때, \n",
    "    2. 기계학습을 할 때 데이터셋을 훈련용/검증용/테스트용으로 샘플링 할 때, \n",
    "    3. 다양한 확률 분포로 부터 데이터를 무작위로 생성해서 시뮬레이션(simulation) 할 때 \n",
    "    => 사용할 수 있는 무작위 난수 만들기(generating random numbers, random sampling)를 이용한다.\n",
    "\n",
    "    ※Python NumPy는 매우 빠르고 효율적으로 무작위 샘플을 만들 수 있는 numpy.random 모듈을 제공합니다.\n",
    "    \n",
    "    얼마나 신기하냐면\n",
    "    random 모듈안에 이산형, 연속형 분포를 따르는 확률분포들이 있는데, 해당 분포에 맞는 parameter를 설정해주면 \n",
    "    파이썬이 알아서 해당 확률분포를 따르는 난수를 출력해주는 것이다.\n",
    "\n",
    "출처: https://rfriend.tistory.com/284 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.98208619 -24.24604412]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [  2.16172855  23.2564833 ]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [  2.16172855  23.2564833 ]\n",
      " [ -6.27341397 -44.69710939]\n",
      " [ -6.09518774 -42.20158939]\n",
      " [ -5.62287892 -39.70587187]]\n",
      "[[ -3.98208619 -24.24604412]\n",
      " [ -5.62287892 -39.70587187]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [ -6.09518774 -42.20158939]\n",
      " [  2.16172855  23.2564833 ]\n",
      " [ -6.27341397 -44.69710939]\n",
      " [ -3.98208619 -24.24604412]\n",
      " [  2.16172855  23.2564833 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.4.1 랜덤 데이터 생성기 구현: Gaussian 분포에 기반 N=1000,10000,1000000\n",
    "    \n",
    "xlis = []\n",
    "ylis = []\n",
    "flis = [] \n",
    "wlis = []\n",
    "\n",
    "def data_set(R,size):\n",
    "     #size==특징\n",
    "    W= np.random.uniform(-R,R,size=size)\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "\n",
    "    for i in range(30):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(W*x+b,1,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "        flis.append(W*x+b)\n",
    "\n",
    "    x=np.array(xlis)\n",
    "    y=np.array(ylis)\n",
    "\n",
    "    result=np.concatenate((x,y),axis=1)\n",
    "\n",
    "        \n",
    "data_set(10,1)\n",
    "plt.scatter(xlis,flis)\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "train_idx=int(result.shape[0]*0.6)\n",
    "dev_idx=int(result.shape[0]*0.1)\n",
    "test_idx=int(result.shape[0]*0.3)\n",
    "\n",
    "train_data_set=result[0:train_idx,:]\n",
    "test_data_set=result[train_idx:train_idx+test_idx,:]\n",
    "dev_data_set=result[train_idx+test_idx:train_idx+test_idx+dev_idx,:]\n",
    "\n",
    "print(test_data_set)\n",
    "test_data_set=np.random.permutation(test_data_set)\n",
    "print(test_data_set)\n",
    "\n",
    "#print(result[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.2 scikit sample: Diabetes\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미니배치 사이즈를 입력(10~100):30\n",
      "epoch size(최대 100):3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-65680303cf64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch size(최대 100):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mminibatch_size\u001b[0m \u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mloss_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_x' is not defined"
     ]
    }
   ],
   "source": [
    "# 2.4.3 Linear regression(Random data_set)\n",
    "''' w,b를 찾아 가는 것. x와 y는 한세트라고 보면 된다\n",
    "    따라서, 셔플을 할 때 x,y세트에다 주는 것이다.'''\n",
    "\n",
    "\n",
    "def data_set(R,size):\n",
    "  #  R=int(input('range: '))\n",
    " #   dimension_size=int(input('size: '))\n",
    "#    sample=int(input('sample: '))\n",
    "\n",
    "    xlis = []\n",
    "    ylis = []\n",
    "    wlis = []\n",
    "    \n",
    "    W= np.random.uniform(-R,R,size=size) #size==특징\n",
    "    b= np.random.uniform(-R,R,size=size)\n",
    "    b= random.choice(b)\n",
    "    \n",
    "    mu=W*x+b\n",
    "    sigma=0.1*R\n",
    "\n",
    "    for i in range(10):\n",
    "        x = np.random.uniform(-R,R,size=size)\n",
    "        y = np.random.normal(mu,sigma,size=size)\n",
    "        xlis.append(x)\n",
    "        ylis.append(y)\n",
    "    \n",
    "x1=np.array(xlis)\n",
    "y1=np.array(ylis)\n",
    "\n",
    "'''\n",
    "plt.scatter(xlis,ylis)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')'''\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    \n",
    "    loss_grad={}    # dJdW, dJdB 저장공간\n",
    "    forward_info={} # 순방향 저장공간\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    loss(X_batch, y_batch, W, b)\n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "    number_minibatch= np.int(np.ceil(X_batch.shape[0]/minibatch_size))\n",
    "    print(number_minibatch)\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "                \n",
    "                \n",
    "            print(loss)\n",
    "            print('=================================')\n",
    "             #print(loss_grad)\n",
    "\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        loss_gradient(train_data_x,train_data_y,train_data_W,train_data_b)\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.3 Linear regression 학습기 Practice\n",
    "\n",
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "b=np.reshape(b,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:860,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:860,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:860,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+train_data_b\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+dev_data_b\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+test_data_b\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=10)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(10,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b, minibatch_size, epoch_size):    \n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "\n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "\n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dJdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dJdB\n",
    "    \n",
    "    print('==================================================================')\n",
    "\n",
    "    print(weights['W'])\n",
    "        #print(loss_grad)\n",
    "'''\n",
    "while 1:\n",
    "    minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "    epoch_size=int(input('epoch size(최대 100):'))\n",
    "    if((10<=minibatch_size <=100) and 1<=epoch_size<=100):\n",
    "        \n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "        break\n",
    "    else:\n",
    "        if(10>=minibatch_size and minibatch_size >=100 and 1<=epoch_size<=100):\n",
    "            print('Out of order about minibatch_size')\n",
    "        elif((10<=minibatch_size <=100) and (0>epoch_size and epoch_size >=100)):\n",
    "            print('Out of order about epoch_size')\n",
    "        else: print('Out of order both')\n",
    "        continue\n",
    "'''\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "for j in range(1,epoch_size+1):\n",
    "    print('*******',j,'번차 epoch*******')\n",
    "    W=np.random.permutation(W)\n",
    "    b=np.random.permutation(b)\n",
    "    x=np.random.permutation(x)\n",
    "    for i in range(1, number_minibatch+1):\n",
    "        print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "        X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "        b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "\n",
    "        loss_gradient(dev_data_x,dev_data_y,dev_data_W,dev_data_b,minibatch_size, epoch_size)\n",
    "        for key in weights.keys():\n",
    "\n",
    "            weights[key]=weights[key]- 0.01 * loss_grad[key]\n",
    "    print(weights['W'])\n",
    "    print('LOSS:',loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W= np.random.uniform(-10,10,size=1000)\n",
    "b= np.random.uniform(-10,10,size=1000)\n",
    "x= np.random.uniform(-10,10,size=1000)\n",
    "\n",
    "W=np.reshape(W,(1000,1))\n",
    "x=np.reshape(x,(1000,1))\n",
    "b1=random.choice(b)\n",
    "\n",
    "train_data_W = W[0:850,:]\n",
    "dev_data_W = W[850:900,:]\n",
    "test_data_W = W[900:1000,:]\n",
    "\n",
    "train_data_x = x[0:850,:]\n",
    "dev_data_x = x[850:900,:]\n",
    "test_data_x = x[900:1000,:]\n",
    "'''\n",
    "train_data_b = b[0:850,:]\n",
    "dev_data_b = b[850:900,:]\n",
    "test_data_b = b[900:1000,:]\n",
    "'''\n",
    "train_data_mu=np.dot(np.transpose(train_data_W),train_data_x)+b1\n",
    "dev_data_mu=np.dot(np.transpose(dev_data_W),dev_data_x)+b1\n",
    "test_data_mu=np.dot(np.transpose(test_data_W),test_data_x)+b1\n",
    "\n",
    "sigma=0.1*10\n",
    "\n",
    "train_data_y=np.random.normal(train_data_mu,sigma,size=850)\n",
    "dev_data_y=np.random.normal(dev_data_mu,sigma,size=50)\n",
    "test_data_y=np.random.normal(test_data_mu,sigma,size=100)\n",
    "\n",
    "train_data_y=np.reshape(train_data_y,(850,1))\n",
    "dev_data_y=np.reshape(dev_data_y,(50,1))\n",
    "test_data_y=np.reshape(test_data_y,(100,1))\n",
    "\n",
    "loss_grad={}    # dJdW, dJdB 저장공간\n",
    "forward_info={} # 순방향 저장공간\n",
    "\n",
    "def loss(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "\n",
    "\n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.mean(np.power(y_batch-f,2))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_gradient(X_batch, y_batch, W, b):\n",
    "    b=random.choice(b)\n",
    "    weights = {'W':W,'B':b} \n",
    "\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[0] == weights['W'].shape[0]\n",
    "    \n",
    "    N=np.dot(np.transpose(X_batch),weights['W'])\n",
    "    f= N+weights['B']\n",
    "    loss=np.sum(np.power(y_batch-f,2))\n",
    "    \n",
    "    \n",
    "    #순방향으로 갈때 중간값을 저장해가면서 진행\n",
    "    forward_info['X']= X_batch\n",
    "    forward_info['N']= N       # \n",
    "    forward_info['f']= f       # 예측값\n",
    "    forward_info['y']= y_batch # 실제값\n",
    "\n",
    "    # 전체코드로 본 도함수 계산과정\n",
    "\n",
    "    batch_size=forward_info['X'].shape[0]\n",
    "    dJdf=-2*(forward_info['y']-forward_info['f'])\n",
    "    '''a(N,B)=N+B. N의 어떤 요소를 1단위 증가시키면 f의 값 역시 1단위 증가 따라서  \n",
    "    dfdN의 모든 요소값이 1인 행렬이 되는 것'''  \n",
    "    dfdN=np.ones_like(forward_info['N']) \n",
    "    dfdB=np.ones_like(forward_info['N'])\n",
    "    dJdN=dJdf*dfdN \n",
    "    dNdW=np.transpose(forward_info['X'],(1,0))\n",
    "    \n",
    "    dJdW=np.dot(dNdW, dJdN)\n",
    "    dLdB=(dJdf*dfdB).sum(axis=0)\n",
    "\n",
    "    loss_grad['W']=dJdW\n",
    "    loss_grad['B']=dLdB\n",
    "\n",
    "    return loss_grad\n",
    "\n",
    "#loss(dev_data_x,dev_data_y,dev_data_W,dev_data_b)\n",
    "\n",
    "\n",
    "def epoch(x,y,W,b,minibatch_size,epoch_size):\n",
    "    number_minibatch= np.int(np.ceil(x.shape[0]/minibatch_size))\n",
    "    weights = {'W':W,'B':b} \n",
    "    data_list=[]\n",
    "    \n",
    "    for j in range(1,epoch_size+1):\n",
    "        print('*******',j,'번차 epoch*******')\n",
    "        W=np.random.permutation(W)\n",
    "        x=np.random.permutation(x)\n",
    "        b1=random.choice(b)\n",
    "\n",
    "        for i in range(1, number_minibatch+1):\n",
    "            #print('\\n',minibatch_size*i-(minibatch_size-1)-1,'~',minibatch_size*i-1,'열')\n",
    "            '''\n",
    "            X_batch1=X_batch[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            b1=b[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            '''\n",
    "            x1=x[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            y1=y[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "            W1=W[minibatch_size*i-(minibatch_size-1)-1:minibatch_size*i]\n",
    "           \n",
    "            loss_gradient(x1,y1,W1,b1)\n",
    "            \n",
    "            for key in weights.keys():\n",
    "                weights[key]=weights[key]- 0.0001 * loss_grad[key]\n",
    "        print('Loss:',loss(x,y,W,b)*0.00001)\n",
    "        data_list.append(loss(x,y,W,b)*0.00001)\n",
    "        \n",
    "    return data_list\n",
    "\n",
    "minibatch_size=int(input('미니배치 사이즈를 입력(10~100):'))\n",
    "epoch_size=int(input('epoch size(최대 100):'))\n",
    "print('Dev')\n",
    "dev_data=epoch(dev_data_x,dev_data_y,dev_data_W,b,minibatch_size,epoch_size)\n",
    "print('\\nTest')\n",
    "Test_data=epoch(test_data_x,test_data_y,test_data_W,test_data_b,minibatch_size,epoch_size)        \n",
    "print('\\nTrain')\n",
    "Train_data=epoch(train_data_x,train_data_y,train_data_W,train_data_b,minibatch_size,epoch_size)\n",
    "\n",
    "plt.plot(dev_data, label='dev_loss')\n",
    "plt.plot(Test_data, label='Test_loss')\n",
    "plt.plot(Train_data, label='Train_loss')\n",
    "plt.xlabel('Max_iter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
